{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Curve Fit! Background CurveFit is a Python package for fitting curves using nonlinear mixed effects. It can be used to do only that if desired. However, due to its current usage for the IHME COVID-19 project , it has modules specifically for evaluating model performance out beyond the range of time observed in the data. Likewise, it has modules for creating uncertainty intervals based on out of sample performance. In our methods documentation we discuss the statistical methods for CurveFit . In our code documentation , we explain the core model code and also the extensions that allow for evaluating model performance and generating uncertainty intervals. NOTE: This documentation is currently under construction and being updated regularly. IHME COVID-19 Project For any IHME COVID-19 related inquiries, please contact covid19@healthdata.org . To see the IHME projections visualization, click here . To read the paper, click here . For FAQs, click here . Please note that this code base makes up only one part of the IHME COVID-19 projection process, in particular the COVID-19 deaths forecasting. Getting Started To clone the repository and get started, you can either do git clone https://github.com/ihmeuw-msca/CurveFit.git cd CurveFit pip install . or use make install . Maintainers Aleksandr Aravkin ( saravkin@uw.edu ) Peng Zheng ( zhengp@uw.edu ) Marlena Bannick ( mbannick@uw.edu ) Jize Zhang ( jizez@uw.edu ) Alexey Sholokov ( aksh@uw.edu ) Bradley Bell ( bradbell@seanet.com )","title":"Home"},{"location":"#welcome-to-curve-fit","text":"","title":"Welcome to Curve Fit!"},{"location":"#background","text":"CurveFit is a Python package for fitting curves using nonlinear mixed effects. It can be used to do only that if desired. However, due to its current usage for the IHME COVID-19 project , it has modules specifically for evaluating model performance out beyond the range of time observed in the data. Likewise, it has modules for creating uncertainty intervals based on out of sample performance. In our methods documentation we discuss the statistical methods for CurveFit . In our code documentation , we explain the core model code and also the extensions that allow for evaluating model performance and generating uncertainty intervals. NOTE: This documentation is currently under construction and being updated regularly.","title":"Background"},{"location":"#ihme-covid-19-project","text":"For any IHME COVID-19 related inquiries, please contact covid19@healthdata.org . To see the IHME projections visualization, click here . To read the paper, click here . For FAQs, click here . Please note that this code base makes up only one part of the IHME COVID-19 projection process, in particular the COVID-19 deaths forecasting.","title":"IHME COVID-19 Project"},{"location":"#getting-started","text":"To clone the repository and get started, you can either do git clone https://github.com/ihmeuw-msca/CurveFit.git cd CurveFit pip install . or use make install .","title":"Getting Started"},{"location":"#maintainers","text":"Aleksandr Aravkin ( saravkin@uw.edu ) Peng Zheng ( zhengp@uw.edu ) Marlena Bannick ( mbannick@uw.edu ) Jize Zhang ( jizez@uw.edu ) Alexey Sholokov ( aksh@uw.edu ) Bradley Bell ( bradbell@seanet.com )","title":"Maintainers"},{"location":"code/","text":"Map of the Code We first start by walking through the core curve fitting model , and then the extensions that make it possible for CurveFit to be used for forecasting over time including pipelines and predictive validity . Core Model curevefit.core Here we will walk through how to use CurveModel . First, here is an example that you can copy and paste into your Python interpreter to run start to finish. import pandas as pd import numpy as np import matplotlib.pyplot as plt from curvefit.core.model import CurveModel from curvefit.core.functions import ln_gaussian_cdf np . random . seed ( 1234 ) # Create example data -- both death rate and log death rate df = pd . DataFrame () df [ 'time' ] = np . arange ( 100 ) df [ 'death_rate' ] = np . exp ( . 1 * ( df . time - 20 )) / ( 1 + np . exp ( . 1 * ( df . time - 20 ))) + \\ np . random . normal ( 0 , 0.1 , size = 100 ) . cumsum () df [ 'ln_death_rate' ] = np . log ( df [ 'death_rate' ]) df [ 'group' ] = 'all' df [ 'intercept' ] = 1.0 # Set up the CurveModel model = CurveModel ( df = df , col_t = 'time' , col_obs = 'ln_death_rate' , col_group = 'group' , col_covs = [[ 'intercept' ], [ 'intercept' ], [ 'intercept' ]], param_names = [ 'alpha' , 'beta' , 'p' ], link_fun = [ lambda x : x , lambda x : x , lambda x : x ], var_link_fun = [ lambda x : x , lambda x : x , lambda x : x ], fun = ln_gaussian_cdf ) # Fit the model to estimate parameters model . fit_params ( fe_init = [ 0 , 0 , 1. ], fe_gprior = [[ 0 , np . inf ], [ 0 , np . inf ], [ 1. , np . inf ]]) # Get predictions y_pred = model . predict ( t = df . time , group_name = df . group . unique () ) # Plot results plt . plot ( df . time , y_pred , '-' ) plt . plot ( df . time , df . ln_death_rate , '.' ) Now we will walk through each of the steps from above and explain how to use them in detail. Setting Up a Model The code for the core curve fitting model is curvefit.core.model.CurveModel . To initialize a CurveModel , you need a pandas data frame and information about what type of model you want to fit. It needs to know which columns represent what and some model parameters. df (pd.DataFrame) : data frame with all available information for the model col_t (str) : the column that indicates the independent variable col_obs (str) : the column that indicates the dependent variable col_covs (list{list{str}}) : list of lists of strings that indicate the covariates to use for each covariate col_group (str) : the column that indicates the group variable (even if you only have one group you must pass a column that indicates group membership) param_names (list{str}) : names of the parameters for your specific functional form (more in functions ) link_fun (list{function}) : list of link functions for each of the parameters var_link_fun (list{function}) : list of functions for the variables including fixed and random effects First, we create sample data frame where time is the independent variable, death_rate is the dependent variable, and group is a variable indicating which group an observation belongs to. In this example, we want to fit to the log erf (also referred to as log Gaussian CDF) functional form (see functions ) with identity link functions for each parameter and identity variable link functions for each parameter. In this example, no parameters have covariates besides an intercept column of 1's. model = CurveModel ( df = df , col_t = 'time' , col_obs = 'ln_death_rate' , col_group = 'group' , col_covs = [[ 'intercept' ], [ 'intercept' ], [ 'intercept' ]], param_names = [ 'alpha' , 'beta' , 'p' ], link_fun = [ lambda x : x , lambda x : x , lambda x : x ], var_link_fun = [ lambda x : x , lambda x : x , lambda x : x ], fun = ln_gaussian_cdf ) Functions The curvefit package has some built-in functions for curves to fit. However, this list is not exhaustive, and you may pass any callable function that takes in t (an independent variable) and params (a list of parameters) to the function to the CurveModel class for the fun argument. What you pass in for param_names in the CurveModel needs to match what the fun callable expects. The available built-in functions in curvefit.core.functions are: The Error Function gaussian_cdf : Gaussian cumulative distribution function gaussian_pdf : Gaussian probability distribution function ln_gaussian_cdf : log Gaussian cumulative distribution function ln_gaussian_pdf : log Gaussian probability distribution function The Expit Function (inverse of the logit function) expit : expit function log_expit : log expit function Please see the functions for information about the parametrization of these functions and how they relate to COVID-19 modeling. Fitting a Model Once you have a model defined, the method fit_params fits the model. At minimum, the only information that model.fit_params needs is initial values for the fixed effects. But there are many optional arguments that you can pass to model.fit_params to inform the optimization process. Below we describe each of these optional arguments. The result of fit_params is stored in CurveModel.result and the parameter estimates in CurveModel.params . Gaussian Priors fe_gprior and re_gprior Each parameter may have an associated Gaussian prior. This is optional and can be passed in as a list of lists. This specification, referring to our example will put Gaussian priors with mean 0 and standard deviation 1. on the alpha parameter, mean 0 and standard deviation 1e-3 on the beta parameter and mean 5 and standard deviation 10. on the p parameter. model . fit_params ( fe_gprior = [[ 0 , 1. ], [ 0 , 1e-3 ], [ 5 , 10. ]]) Likewise, you may have random effects Gaussian priors using the argument re_gprior , which has the same shape as fe_gprior , but refers to the random effects. For the specifications of fixed and random effects, please see the methods . Constraints fe_bounds and re_bounds You can also include parameter constraints for each of the fixed effects and the random effects. They are included as a list of lists. This specification, referring to our example , will bound all of the fixed effects between 0 and 100. and the random effects between -1 and 1. model . fit_params ( fe_bounds = [[ 0. , 100. ], [ 0. , 100. ], [ 0. , 100. ]], re_bounds = [[ - 1. , 1. ], [ - 1. , 1. ], [ - 1. , 1. ]] ) If you do not want to include random effects, set the bounds to be exactly 0. Please see more information on constraints in the methods . Initialization The optimization routine will perform better with smart starting values for the parameters. Initial values for the fixed effects, fe_init , are required and is passed in as a numpy array of the same length as your parameters. The initial values for the random effects, re_init , are passed in as a numpy array ordered by the group name and parameters. For example, if you had two groups in the model, the following would initialize the fixed effects at 1., 1., 1., and the random effects at -0.5, -0.5, -0.5, for the first group and 0.5, 0.5, 0.5, for the second group. import numpy as np model . fit_params ( fe_init = np . array ([ 1. , 1. , 1. ]), re_init = np . array ([ - 0.5 , - 0.5 , - 0.5 , 0.5 , 0.5 , 0.5 ]) ) There is an optional flag, smart_initialize that if True will run a model individually for each group in the model and use their fixed effects estimates to inform the initial values for both fixed and random effects of the mixed model that you want to fit. Optimization The optimization uses scipy.optimize.minimize and the \"L-BFGS-B\" which has a list of options that you can pass to it. These keyword options can be passed to the minimize function using the options argument. For example, the following would perform a maximum of 500 iterations and require an objective function tolerance of 1e-10. model . fit_params ( options = { 'ftol' : 1e-10 , 'maxiter' : 500 } ) If you have indicated that you want the model to do smart initialization with smart_initialize = True , then you can optionally pass a dictionary of smart_init_options to override the options just for the group-specific initial fits. Otherwise it will use all of the options in both the group-specific and overall fits. Please see scipy.optimize.minimize for more options. Please see the methods for more information about the optimization procedure. Obtaining Predictions from a Model To obtain predictions from a model that has been fit, use the method CurveModel.predict . The predict function needs to know which values of the independent variable you want to predict for, which group you want to predict for, and optionally, which space you want to predict in. For example, you might want to predict in ln_gaussian_cdf space but make predictions in ln_gaussian_pdf space. This is only possible for functions that are related to one another (see the functions section). Continuing with our example , the following call would make predictions at all of the times in the original data frame, for group \"all\" . y_pred = model . predict ( t = df . time , group_name = df . group . unique () ) Model Pipelines curvefit.pipelines To customize the modeling process for a specific problem, and integrate the core model with predictive validity and uncertainty, there is a class curvefit.pipelines._pipeline.ModelPipeline that sets up the structure. Each file in curvefit.piplelines subclasses this ModelPipeline to have different types of modeling processes. A ModelPipeline needs to get much of the same information that is passed to CurveModel . The additional arguments that it needs are predict_space (callable) : a curvefit.core.functions function that matches what space you want to do predictive validity in all_cov_names (list{str}) : a list of all the covariate names that will be used obs_se_func (callable) : in place of col_obs_se we now need to define a function that produces the standard error as a function of the independent variable The overall run() method that will be used in ModelPipeline does the following things: ModelPipeline.run_init_model() : runs aspects of the model that will not be re-run during predictive validity and/or stores information for use later ModelPipeline.run_predictive_validity() : runs predictive validity, described here ModelPipeline.fit_residuals() : fits residuals from predictive validity ModelPipeline.create_draws() : creates random realizations of the mean function that for the uncertainty intervals Each subclass of ModelPipeline has different requirements, each of which are described in their respective docstrings. Available classes and a brief description of what they do are below: BasicModel : Runs one model jointly with all groups. BasicModelWithInit : Runs all models separately to do a smart initialization of the fixed and random effects, and then runs a joint model with all groups. TightLooseModel : Runs four models with different combinations of settings (one setting should be \"tight\", meaning that it follows the prior closely and one \"loose\" meaning that it follows the prior less closely) and covariate models (can place the covariates on different parameters across models -- by default one is called the \"beta\" model and one is called the \"p\" model referring to which parameter has covariates). The \"tight\" and \"loose\" model predictions are blended within each covariate covariate model by using a convex combination of the predictions over time. Then the two covariate models are averaged together with pre-specified weights. APModel : Runs group-specific models and introduces a functional prior on the log of the alpha and beta parameters for the erf family of functions. PreConditionedAPModel : Runs like an APModel with the erf family but dynamically adjusts the bounds for the fixed effects of group-specific models based on preconditioning that flags groups that still have an exponential rise in the dependent variable with respect to the independent variable. Predictive Validity curvefit.pv","title":"Code"},{"location":"code/#map-of-the-code","text":"We first start by walking through the core curve fitting model , and then the extensions that make it possible for CurveFit to be used for forecasting over time including pipelines and predictive validity .","title":"Map of the Code"},{"location":"code/#core-model","text":"curevefit.core Here we will walk through how to use CurveModel . First, here is an example that you can copy and paste into your Python interpreter to run start to finish. import pandas as pd import numpy as np import matplotlib.pyplot as plt from curvefit.core.model import CurveModel from curvefit.core.functions import ln_gaussian_cdf np . random . seed ( 1234 ) # Create example data -- both death rate and log death rate df = pd . DataFrame () df [ 'time' ] = np . arange ( 100 ) df [ 'death_rate' ] = np . exp ( . 1 * ( df . time - 20 )) / ( 1 + np . exp ( . 1 * ( df . time - 20 ))) + \\ np . random . normal ( 0 , 0.1 , size = 100 ) . cumsum () df [ 'ln_death_rate' ] = np . log ( df [ 'death_rate' ]) df [ 'group' ] = 'all' df [ 'intercept' ] = 1.0 # Set up the CurveModel model = CurveModel ( df = df , col_t = 'time' , col_obs = 'ln_death_rate' , col_group = 'group' , col_covs = [[ 'intercept' ], [ 'intercept' ], [ 'intercept' ]], param_names = [ 'alpha' , 'beta' , 'p' ], link_fun = [ lambda x : x , lambda x : x , lambda x : x ], var_link_fun = [ lambda x : x , lambda x : x , lambda x : x ], fun = ln_gaussian_cdf ) # Fit the model to estimate parameters model . fit_params ( fe_init = [ 0 , 0 , 1. ], fe_gprior = [[ 0 , np . inf ], [ 0 , np . inf ], [ 1. , np . inf ]]) # Get predictions y_pred = model . predict ( t = df . time , group_name = df . group . unique () ) # Plot results plt . plot ( df . time , y_pred , '-' ) plt . plot ( df . time , df . ln_death_rate , '.' ) Now we will walk through each of the steps from above and explain how to use them in detail.","title":"Core Model"},{"location":"code/#setting-up-a-model","text":"The code for the core curve fitting model is curvefit.core.model.CurveModel . To initialize a CurveModel , you need a pandas data frame and information about what type of model you want to fit. It needs to know which columns represent what and some model parameters. df (pd.DataFrame) : data frame with all available information for the model col_t (str) : the column that indicates the independent variable col_obs (str) : the column that indicates the dependent variable col_covs (list{list{str}}) : list of lists of strings that indicate the covariates to use for each covariate col_group (str) : the column that indicates the group variable (even if you only have one group you must pass a column that indicates group membership) param_names (list{str}) : names of the parameters for your specific functional form (more in functions ) link_fun (list{function}) : list of link functions for each of the parameters var_link_fun (list{function}) : list of functions for the variables including fixed and random effects First, we create sample data frame where time is the independent variable, death_rate is the dependent variable, and group is a variable indicating which group an observation belongs to. In this example, we want to fit to the log erf (also referred to as log Gaussian CDF) functional form (see functions ) with identity link functions for each parameter and identity variable link functions for each parameter. In this example, no parameters have covariates besides an intercept column of 1's. model = CurveModel ( df = df , col_t = 'time' , col_obs = 'ln_death_rate' , col_group = 'group' , col_covs = [[ 'intercept' ], [ 'intercept' ], [ 'intercept' ]], param_names = [ 'alpha' , 'beta' , 'p' ], link_fun = [ lambda x : x , lambda x : x , lambda x : x ], var_link_fun = [ lambda x : x , lambda x : x , lambda x : x ], fun = ln_gaussian_cdf )","title":"Setting Up a Model"},{"location":"code/#functions","text":"The curvefit package has some built-in functions for curves to fit. However, this list is not exhaustive, and you may pass any callable function that takes in t (an independent variable) and params (a list of parameters) to the function to the CurveModel class for the fun argument. What you pass in for param_names in the CurveModel needs to match what the fun callable expects. The available built-in functions in curvefit.core.functions are: The Error Function gaussian_cdf : Gaussian cumulative distribution function gaussian_pdf : Gaussian probability distribution function ln_gaussian_cdf : log Gaussian cumulative distribution function ln_gaussian_pdf : log Gaussian probability distribution function The Expit Function (inverse of the logit function) expit : expit function log_expit : log expit function Please see the functions for information about the parametrization of these functions and how they relate to COVID-19 modeling.","title":"Functions"},{"location":"code/#fitting-a-model","text":"Once you have a model defined, the method fit_params fits the model. At minimum, the only information that model.fit_params needs is initial values for the fixed effects. But there are many optional arguments that you can pass to model.fit_params to inform the optimization process. Below we describe each of these optional arguments. The result of fit_params is stored in CurveModel.result and the parameter estimates in CurveModel.params .","title":"Fitting a Model"},{"location":"code/#gaussian-priors","text":"fe_gprior and re_gprior Each parameter may have an associated Gaussian prior. This is optional and can be passed in as a list of lists. This specification, referring to our example will put Gaussian priors with mean 0 and standard deviation 1. on the alpha parameter, mean 0 and standard deviation 1e-3 on the beta parameter and mean 5 and standard deviation 10. on the p parameter. model . fit_params ( fe_gprior = [[ 0 , 1. ], [ 0 , 1e-3 ], [ 5 , 10. ]]) Likewise, you may have random effects Gaussian priors using the argument re_gprior , which has the same shape as fe_gprior , but refers to the random effects. For the specifications of fixed and random effects, please see the methods .","title":"Gaussian Priors"},{"location":"code/#constraints","text":"fe_bounds and re_bounds You can also include parameter constraints for each of the fixed effects and the random effects. They are included as a list of lists. This specification, referring to our example , will bound all of the fixed effects between 0 and 100. and the random effects between -1 and 1. model . fit_params ( fe_bounds = [[ 0. , 100. ], [ 0. , 100. ], [ 0. , 100. ]], re_bounds = [[ - 1. , 1. ], [ - 1. , 1. ], [ - 1. , 1. ]] ) If you do not want to include random effects, set the bounds to be exactly 0. Please see more information on constraints in the methods .","title":"Constraints"},{"location":"code/#initialization","text":"The optimization routine will perform better with smart starting values for the parameters. Initial values for the fixed effects, fe_init , are required and is passed in as a numpy array of the same length as your parameters. The initial values for the random effects, re_init , are passed in as a numpy array ordered by the group name and parameters. For example, if you had two groups in the model, the following would initialize the fixed effects at 1., 1., 1., and the random effects at -0.5, -0.5, -0.5, for the first group and 0.5, 0.5, 0.5, for the second group. import numpy as np model . fit_params ( fe_init = np . array ([ 1. , 1. , 1. ]), re_init = np . array ([ - 0.5 , - 0.5 , - 0.5 , 0.5 , 0.5 , 0.5 ]) ) There is an optional flag, smart_initialize that if True will run a model individually for each group in the model and use their fixed effects estimates to inform the initial values for both fixed and random effects of the mixed model that you want to fit.","title":"Initialization"},{"location":"code/#optimization","text":"The optimization uses scipy.optimize.minimize and the \"L-BFGS-B\" which has a list of options that you can pass to it. These keyword options can be passed to the minimize function using the options argument. For example, the following would perform a maximum of 500 iterations and require an objective function tolerance of 1e-10. model . fit_params ( options = { 'ftol' : 1e-10 , 'maxiter' : 500 } ) If you have indicated that you want the model to do smart initialization with smart_initialize = True , then you can optionally pass a dictionary of smart_init_options to override the options just for the group-specific initial fits. Otherwise it will use all of the options in both the group-specific and overall fits. Please see scipy.optimize.minimize for more options. Please see the methods for more information about the optimization procedure.","title":"Optimization"},{"location":"code/#obtaining-predictions-from-a-model","text":"To obtain predictions from a model that has been fit, use the method CurveModel.predict . The predict function needs to know which values of the independent variable you want to predict for, which group you want to predict for, and optionally, which space you want to predict in. For example, you might want to predict in ln_gaussian_cdf space but make predictions in ln_gaussian_pdf space. This is only possible for functions that are related to one another (see the functions section). Continuing with our example , the following call would make predictions at all of the times in the original data frame, for group \"all\" . y_pred = model . predict ( t = df . time , group_name = df . group . unique () )","title":"Obtaining Predictions from a Model"},{"location":"code/#model-pipelines","text":"curvefit.pipelines To customize the modeling process for a specific problem, and integrate the core model with predictive validity and uncertainty, there is a class curvefit.pipelines._pipeline.ModelPipeline that sets up the structure. Each file in curvefit.piplelines subclasses this ModelPipeline to have different types of modeling processes. A ModelPipeline needs to get much of the same information that is passed to CurveModel . The additional arguments that it needs are predict_space (callable) : a curvefit.core.functions function that matches what space you want to do predictive validity in all_cov_names (list{str}) : a list of all the covariate names that will be used obs_se_func (callable) : in place of col_obs_se we now need to define a function that produces the standard error as a function of the independent variable The overall run() method that will be used in ModelPipeline does the following things: ModelPipeline.run_init_model() : runs aspects of the model that will not be re-run during predictive validity and/or stores information for use later ModelPipeline.run_predictive_validity() : runs predictive validity, described here ModelPipeline.fit_residuals() : fits residuals from predictive validity ModelPipeline.create_draws() : creates random realizations of the mean function that for the uncertainty intervals Each subclass of ModelPipeline has different requirements, each of which are described in their respective docstrings. Available classes and a brief description of what they do are below: BasicModel : Runs one model jointly with all groups. BasicModelWithInit : Runs all models separately to do a smart initialization of the fixed and random effects, and then runs a joint model with all groups. TightLooseModel : Runs four models with different combinations of settings (one setting should be \"tight\", meaning that it follows the prior closely and one \"loose\" meaning that it follows the prior less closely) and covariate models (can place the covariates on different parameters across models -- by default one is called the \"beta\" model and one is called the \"p\" model referring to which parameter has covariates). The \"tight\" and \"loose\" model predictions are blended within each covariate covariate model by using a convex combination of the predictions over time. Then the two covariate models are averaged together with pre-specified weights. APModel : Runs group-specific models and introduces a functional prior on the log of the alpha and beta parameters for the erf family of functions. PreConditionedAPModel : Runs like an APModel with the erf family but dynamically adjusts the bounds for the fixed effects of group-specific models based on preconditioning that flags groups that still have an exponential rise in the dependent variable with respect to the independent variable.","title":"Model Pipelines"},{"location":"code/#predictive-validity","text":"curvefit.pv","title":"Predictive Validity"},{"location":"methods/","text":"Overview CurveFit is an extendable nonlinear mixed effects model for fitting curves. The main application in this development is COVID-19 forecasting, so that the curves we consider are variants of logistic models. However the interface allows any user-specified parametrized family. Parametrized curves have several key features that make them useful for forecasting: We can capture key signals from noisy data. Parameters are interpretable, and can be modeled using covariates in a transparent way. Parametric forms allow for more stable inversion approaches, for current and future work. Parametric functions impose rigid assumptions that make forecasting more stable. COVID-19 functional forms We considered two functional forms so far when modeling the COVID-19 epidemic. Generalized Logistic: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp(-\\alpha(t-\\beta))} Generalized Gaussian Cumulative Distribution Function f(t; \\alpha, \\beta, p) = \\frac{p}{2}\\left(\\Psi(\\alpha(t-\\beta)\\right) = \\frac{p}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha(t-\\beta)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) Each form has comparable fundamental parameters: Level p : Controls the ultimate level. Slope \\alpha : Controls speed of infection. Inflection \\beta : Time at which the rate of change is maximal. We can fit these parameters to data, but this by itself does not account for covariates, and cannot connect different locations together. The next section therefore specifies statistical models that do this. Statistical Model Statistical assumptions link covariates across locations. Key aspects are the following: Parameters may be influenced by covariates, e.g. those that reflect social distancing Parameters may be modeled in a different space, e.g. p, \\alpha are non-negative Parameters and covariate multipliers may be location-specific, with assumptions placed on their variation. CurveFit specification is tailored to these three requirements. Every parameter in any functional form can be specified through a link function, covariates, fixed, and random effects. The final estimation problem is a nonlinear mixed effects model, with user-specified priors on fixed and random effects. For example, consider the ERF functional form with covariates \\alpha, \\beta, p . Assume we are fitting data in log-cumulative-death-rate space. Input data are: S_j : social distancing covariate value at location j y_j^t : cumulative death rate in location j at time t We specify the statistical model as follows: Measurement model: \\begin{aligned} \\log(y_j^t) &= \\frac{p_j}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha_j(t-\\beta_j)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) + \\epsilon_{t,j} \\\\ \\epsilon_{t,j} & \\sim N(0, V_t) \\end{aligned} \\beta -model specification: \\begin{aligned} \\beta_j &= \\beta + \\gamma_j S_j + \\epsilon_j^\\beta \\\\ \\gamma_j &\\sim N(\\overline \\gamma, V_\\gamma) \\\\ \\epsilon_j^\\beta &\\sim N(0, V_\\beta) \\end{aligned} \\alpha -model specification: \\begin{aligned} \\alpha_j &= \\exp(\\alpha + u_j^\\alpha) \\\\ u_{\\alpha, j} & \\sim N(0, V_\\alpha) \\end{aligned} p -model specification: \\begin{aligned} p_j & = \\exp(p + u_j^p) \\\\ u_{p,j} & \\sim N(0, V_p) \\end{aligned} In this example, the user specifies prior mean \\overline \\gamma variance parameters V_t, V_\\gamma, V_\\beta, V_\\alpha, V_p . CurveFit estimates: fixed effects \\alpha, \\beta, p random effects \\{\\gamma_j, u_j^\\alpha, u_j^\\beta, u_j^p\\} Exponential link functions are used to model non-negative parameters \\alpha, p . Constraints Simple bound constraints on parameters can be used to make the model more robust. For any fixed or random effect, the user can enter simple bound constraints of the form L \\leq \\theta \\leq U. The parameters returned by CurveFit are guaranteed to satisfy these simple bounds. Optimization Procedure The optimization problem we obtain from specifying functional forms, priors, and constraints on all parameters is a bound-constrained nonlinear least squares problem. We explain the solver, derivative computation, and initialization procedure below. Solver We solve the problem using L-BFGS-B . The L-BFGS-B algorithm uses gradients to build a Hessian approximation, and efficiently uses that approximation and projected gradient method onto the bound constraints to identify parameter spaces over which solutions can be efficiently found, see the paper . It is a standard and robust algorithm that's well suited to the task. Derivatives We do not explicitly compute derivatives of the nonlinear least squares objective induced from the problem specification. Instead, we use the complex step method to do this. The complex step method is a simple example of Automatic Differentiation , that is, it can provide machine precision derivatives at the cost of a function evaluation. This is very useful given the flexibility on functional forms. Uncertainty Currently CurveFit uses model-based uncertainty, with out-of-sample approaches under development. Predictive Validity-Based Uncertainty We have a tool that evaluates predictive validity out of sample for the model forecasts. It iteratively holds out data points starting with only one data point used for fitting and adding them back in one by one, comparing the predictions with the observed data. The standard deviation observed for these residuals -- along the dimensions of how much data the model sees and how far out the model needs to predict into the future -- are then used to simulate draws (random realizations of the mean function) that can be used to construct uncertainty intervals. This approach is orthogonal to model-based uncertainty described below. Model-Based Uncertainty We partition model-based uncertainty into estimates coming from fixed and random components. Fixed effects capture the variation of the mean effects, and random effects uncertainty captures the variation across locations. Fixed Effects For any estimator obtained by solving a nonlinear least squares problem, we can use the Fisher information matrix to get an asymptotic approximation to the uncertainty. Let \\hat{ \\theta} = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta^T W^{-1} \\theta + \\frac{1}{2 \\sigma^2} \\|\\Sigma^{-1/2} (y - f( \\theta; X))\\|^2 where W is any prior variance and \\Sigma is the variance of observations. Then our approximation for the variance matrix of the estimate is given by V(\\hat \\theta) = \\mathcal{I}(\\theta)^{-1} = \\left(J_{\\hat \\theta}^T \\Sigma^{-1} J_{\\hat \\theta} + W^{-1} \\right)^{-1} where J_{\\hat{ \\theta}} := \\nabla_{ \\theta} f( \\theta; X) is the Jacobian matrix evaluated at \\theta = \\hat \\theta . The Jacobian is also computed using the complex step method. Random effects To obtain the variance of the random effects, we derive an empirical variance matrix across locations. Given a set of zero mean random effect estimates \\{v_j\\} , with each v_j a vector of k of random effect types, we get an empirical matrix V_0 \\in \\mathbb{R}^{k\\times k} by V_0 = \\frac{1}{n}\\sum_{j=1}^N v_j v_j^T To obtain posterior uncertainty for each specific location, we use the empirical V_0 as a prior, and any data at the location as the measurement model, and re-fit the location: \\hat{ \\theta}_i = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta_i^T V_0^{-1}\\theta_i + \\frac{1}{2 \\sigma^2} \\| \\Sigma_i^{-1/2}(y_i - f_i( \\theta_i; X_i))\\|^2 Within each location, this is analogous to the fixed effects analysis. The location-specific uncertainty is then estimated from the same Fisher information analysis: V_i({\\hat \\theta}) = ( J_i^T \\Sigma_i ^{-1} J_i + V_0^{-1})^{-1}.","title":"Methods"},{"location":"methods/#overview","text":"CurveFit is an extendable nonlinear mixed effects model for fitting curves. The main application in this development is COVID-19 forecasting, so that the curves we consider are variants of logistic models. However the interface allows any user-specified parametrized family. Parametrized curves have several key features that make them useful for forecasting: We can capture key signals from noisy data. Parameters are interpretable, and can be modeled using covariates in a transparent way. Parametric forms allow for more stable inversion approaches, for current and future work. Parametric functions impose rigid assumptions that make forecasting more stable.","title":"Overview"},{"location":"methods/#covid-19-functional-forms","text":"We considered two functional forms so far when modeling the COVID-19 epidemic. Generalized Logistic: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp(-\\alpha(t-\\beta))} Generalized Gaussian Cumulative Distribution Function f(t; \\alpha, \\beta, p) = \\frac{p}{2}\\left(\\Psi(\\alpha(t-\\beta)\\right) = \\frac{p}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha(t-\\beta)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) Each form has comparable fundamental parameters: Level p : Controls the ultimate level. Slope \\alpha : Controls speed of infection. Inflection \\beta : Time at which the rate of change is maximal. We can fit these parameters to data, but this by itself does not account for covariates, and cannot connect different locations together. The next section therefore specifies statistical models that do this.","title":"COVID-19 functional forms"},{"location":"methods/#statistical-model","text":"Statistical assumptions link covariates across locations. Key aspects are the following: Parameters may be influenced by covariates, e.g. those that reflect social distancing Parameters may be modeled in a different space, e.g. p, \\alpha are non-negative Parameters and covariate multipliers may be location-specific, with assumptions placed on their variation. CurveFit specification is tailored to these three requirements. Every parameter in any functional form can be specified through a link function, covariates, fixed, and random effects. The final estimation problem is a nonlinear mixed effects model, with user-specified priors on fixed and random effects. For example, consider the ERF functional form with covariates \\alpha, \\beta, p . Assume we are fitting data in log-cumulative-death-rate space. Input data are: S_j : social distancing covariate value at location j y_j^t : cumulative death rate in location j at time t We specify the statistical model as follows: Measurement model: \\begin{aligned} \\log(y_j^t) &= \\frac{p_j}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha_j(t-\\beta_j)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) + \\epsilon_{t,j} \\\\ \\epsilon_{t,j} & \\sim N(0, V_t) \\end{aligned} \\beta -model specification: \\begin{aligned} \\beta_j &= \\beta + \\gamma_j S_j + \\epsilon_j^\\beta \\\\ \\gamma_j &\\sim N(\\overline \\gamma, V_\\gamma) \\\\ \\epsilon_j^\\beta &\\sim N(0, V_\\beta) \\end{aligned} \\alpha -model specification: \\begin{aligned} \\alpha_j &= \\exp(\\alpha + u_j^\\alpha) \\\\ u_{\\alpha, j} & \\sim N(0, V_\\alpha) \\end{aligned} p -model specification: \\begin{aligned} p_j & = \\exp(p + u_j^p) \\\\ u_{p,j} & \\sim N(0, V_p) \\end{aligned} In this example, the user specifies prior mean \\overline \\gamma variance parameters V_t, V_\\gamma, V_\\beta, V_\\alpha, V_p . CurveFit estimates: fixed effects \\alpha, \\beta, p random effects \\{\\gamma_j, u_j^\\alpha, u_j^\\beta, u_j^p\\} Exponential link functions are used to model non-negative parameters \\alpha, p .","title":"Statistical Model"},{"location":"methods/#constraints","text":"Simple bound constraints on parameters can be used to make the model more robust. For any fixed or random effect, the user can enter simple bound constraints of the form L \\leq \\theta \\leq U. The parameters returned by CurveFit are guaranteed to satisfy these simple bounds.","title":"Constraints"},{"location":"methods/#optimization-procedure","text":"The optimization problem we obtain from specifying functional forms, priors, and constraints on all parameters is a bound-constrained nonlinear least squares problem. We explain the solver, derivative computation, and initialization procedure below.","title":"Optimization Procedure"},{"location":"methods/#solver","text":"We solve the problem using L-BFGS-B . The L-BFGS-B algorithm uses gradients to build a Hessian approximation, and efficiently uses that approximation and projected gradient method onto the bound constraints to identify parameter spaces over which solutions can be efficiently found, see the paper . It is a standard and robust algorithm that's well suited to the task.","title":"Solver"},{"location":"methods/#derivatives","text":"We do not explicitly compute derivatives of the nonlinear least squares objective induced from the problem specification. Instead, we use the complex step method to do this. The complex step method is a simple example of Automatic Differentiation , that is, it can provide machine precision derivatives at the cost of a function evaluation. This is very useful given the flexibility on functional forms.","title":"Derivatives"},{"location":"methods/#uncertainty","text":"Currently CurveFit uses model-based uncertainty, with out-of-sample approaches under development.","title":"Uncertainty"},{"location":"methods/#predictive-validity-based-uncertainty","text":"We have a tool that evaluates predictive validity out of sample for the model forecasts. It iteratively holds out data points starting with only one data point used for fitting and adding them back in one by one, comparing the predictions with the observed data. The standard deviation observed for these residuals -- along the dimensions of how much data the model sees and how far out the model needs to predict into the future -- are then used to simulate draws (random realizations of the mean function) that can be used to construct uncertainty intervals. This approach is orthogonal to model-based uncertainty described below.","title":"Predictive Validity-Based Uncertainty"},{"location":"methods/#model-based-uncertainty","text":"We partition model-based uncertainty into estimates coming from fixed and random components. Fixed effects capture the variation of the mean effects, and random effects uncertainty captures the variation across locations. Fixed Effects For any estimator obtained by solving a nonlinear least squares problem, we can use the Fisher information matrix to get an asymptotic approximation to the uncertainty. Let \\hat{ \\theta} = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta^T W^{-1} \\theta + \\frac{1}{2 \\sigma^2} \\|\\Sigma^{-1/2} (y - f( \\theta; X))\\|^2 where W is any prior variance and \\Sigma is the variance of observations. Then our approximation for the variance matrix of the estimate is given by V(\\hat \\theta) = \\mathcal{I}(\\theta)^{-1} = \\left(J_{\\hat \\theta}^T \\Sigma^{-1} J_{\\hat \\theta} + W^{-1} \\right)^{-1} where J_{\\hat{ \\theta}} := \\nabla_{ \\theta} f( \\theta; X) is the Jacobian matrix evaluated at \\theta = \\hat \\theta . The Jacobian is also computed using the complex step method. Random effects To obtain the variance of the random effects, we derive an empirical variance matrix across locations. Given a set of zero mean random effect estimates \\{v_j\\} , with each v_j a vector of k of random effect types, we get an empirical matrix V_0 \\in \\mathbb{R}^{k\\times k} by V_0 = \\frac{1}{n}\\sum_{j=1}^N v_j v_j^T To obtain posterior uncertainty for each specific location, we use the empirical V_0 as a prior, and any data at the location as the measurement model, and re-fit the location: \\hat{ \\theta}_i = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta_i^T V_0^{-1}\\theta_i + \\frac{1}{2 \\sigma^2} \\| \\Sigma_i^{-1/2}(y_i - f_i( \\theta_i; X_i))\\|^2 Within each location, this is analogous to the fixed effects analysis. The location-specific uncertainty is then estimated from the same Fisher information analysis: V_i({\\hat \\theta}) = ( J_i^T \\Sigma_i ^{-1} J_i + V_0^{-1})^{-1}.","title":"Model-Based Uncertainty"},{"location":"updates/","text":"","title":"Release Notes"},{"location":"extract_md/covariate_xam/","text":"Using Covariates Generalized Gaussian Cumulative Distribution Function The model for the mean of the data for this example is: f(t; \\alpha, \\beta, p) = \\frac{p}{2} \\left( 1 + \\frac{2}{\\pi} \\int_0^{\\alpha ( t - \\beta )} \\exp( - \\tau^2 ) d \\tau \\right) where \\alpha , \\beta , and p are unknown parameters. In addition, the value of \\beta depends on covariate. Fixed Effects We use the notation a , b , c and \\phi for the fixed effects corresponding to the parameters \\alpha , \\beta , and p . For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b + c \\cdot s \\\\ p & = \\exp( \\phi ) \\end{aligned} where s is the social distance covariate. Random effects For this example the random effects are constrained to be zero. Social Distance For this simulation, the social distance covariate has two values: s_i = \\left\\{ \\begin{array}{ll} 0 & \\mbox{if} \\; i < n_D / 2 \\\\ 1 & \\mbox{otherwise} \\end{array} \\right. Simulated data Problem Settings The following settings are used to simulate the data and check that the solution is correct: import math n_data = 21 # number simulated measurements to generate b_true = 20.0 # b used to simulate data a_true = math . log ( 2.0 / b_true ) # a used to simulate data c_true = 1.0 / b_true # c used to simulate data phi_true = math . log ( 0.1 ) # phi used to simulate data rel_tol = 1e-5 # relative tolerance used to check optimal solution The fixed effects a , b , c , and \\phi are initialized so that they correspond to the true fixed effects divided by three. Time Grid A grid of n_data points in time, t_i , where t_i = b_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is b_T . Measurement Values We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , b_T + c_T \\cdot s_i , p_T ) Note that when we do the fitting, we model each data point as having noise. Example Source Code # ------------------------------------------------------------------------- import sys import pandas import numpy import scipy import pdb import sandbox sandbox . path () import curvefit from curvefit.core.model import CurveModel # # model for the mean of the data def gaussian_cdf ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return 0.5 * p * ( 1.0 + scipy . special . erf ( alpha * ( t - beta ) ) ) # # link function used for beta def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # inverse of function used for alpha, p def log_fun ( x ) : return numpy . log ( x ) # # true value for fixed effects fe_true = numpy . array ( [ a_true , b_true , c_true , phi_true ] ) num_fe = len ( fe_true ) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * b_true / ( n_data - 1 ) social_distance = numpy . zeros ( n_data , dtype = float ) params_true = numpy . zeros (( n_data , 3 ), dtype = float ) alpha_true = numpy . exp ( a_true ) p_true = numpy . exp ( phi_true ) for i in range ( n_data ) : social_distance [ i ] = 0 if i < n_data / 2.0 else 1 beta_true = b_true + c_true * social_distance [ i ] params_true [ i ] = [ alpha_true , beta_true , p_true ] params_true = numpy . transpose ( params_true ) measurement_value = gaussian_cdf ( independent_var , params_true ) measurement_std = n_data * [ 0.1 ] cov_one = n_data * [ 1.0 ] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'cov_one' : cov_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = [ [ 'cov_one' ], [ 'cov_one' , 'social_distance' ], [ 'cov_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = gaussian_cdf col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # fe_init = fe_true / 3.0 re_init = numpy . zeros ( num_fe ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ 0.0 , 0.0 ] ] * num_fe options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , options = options ) fe_estimate = curve_model . result . x [: num_fe ] # ------------------------------------------------------------------------- # check result for i in range ( num_fe ) : rel_error = fe_estimate [ i ] / fe_true [ i ] - 1.0 assert abs ( rel_error ) < rel_tol # print ( 'covariate.py: OK' ) sys . exit ( 0 )","title":"covariate_xam"},{"location":"extract_md/covariate_xam/#using-covariates","text":"","title":"Using Covariates"},{"location":"extract_md/covariate_xam/#generalized-gaussian-cumulative-distribution-function","text":"The model for the mean of the data for this example is: f(t; \\alpha, \\beta, p) = \\frac{p}{2} \\left( 1 + \\frac{2}{\\pi} \\int_0^{\\alpha ( t - \\beta )} \\exp( - \\tau^2 ) d \\tau \\right) where \\alpha , \\beta , and p are unknown parameters. In addition, the value of \\beta depends on covariate.","title":"Generalized Gaussian Cumulative Distribution Function"},{"location":"extract_md/covariate_xam/#fixed-effects","text":"We use the notation a , b , c and \\phi for the fixed effects corresponding to the parameters \\alpha , \\beta , and p . For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b + c \\cdot s \\\\ p & = \\exp( \\phi ) \\end{aligned} where s is the social distance covariate.","title":"Fixed Effects"},{"location":"extract_md/covariate_xam/#random-effects","text":"For this example the random effects are constrained to be zero.","title":"Random effects"},{"location":"extract_md/covariate_xam/#social-distance","text":"For this simulation, the social distance covariate has two values: s_i = \\left\\{ \\begin{array}{ll} 0 & \\mbox{if} \\; i < n_D / 2 \\\\ 1 & \\mbox{otherwise} \\end{array} \\right.","title":"Social Distance"},{"location":"extract_md/covariate_xam/#simulated-data","text":"","title":"Simulated data"},{"location":"extract_md/covariate_xam/#problem-settings","text":"The following settings are used to simulate the data and check that the solution is correct: import math n_data = 21 # number simulated measurements to generate b_true = 20.0 # b used to simulate data a_true = math . log ( 2.0 / b_true ) # a used to simulate data c_true = 1.0 / b_true # c used to simulate data phi_true = math . log ( 0.1 ) # phi used to simulate data rel_tol = 1e-5 # relative tolerance used to check optimal solution The fixed effects a , b , c , and \\phi are initialized so that they correspond to the true fixed effects divided by three.","title":"Problem Settings"},{"location":"extract_md/covariate_xam/#time-grid","text":"A grid of n_data points in time, t_i , where t_i = b_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is b_T .","title":"Time Grid"},{"location":"extract_md/covariate_xam/#measurement-values","text":"We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , b_T + c_T \\cdot s_i , p_T ) Note that when we do the fitting, we model each data point as having noise.","title":"Measurement Values"},{"location":"extract_md/covariate_xam/#example-source-code","text":"# ------------------------------------------------------------------------- import sys import pandas import numpy import scipy import pdb import sandbox sandbox . path () import curvefit from curvefit.core.model import CurveModel # # model for the mean of the data def gaussian_cdf ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return 0.5 * p * ( 1.0 + scipy . special . erf ( alpha * ( t - beta ) ) ) # # link function used for beta def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # inverse of function used for alpha, p def log_fun ( x ) : return numpy . log ( x ) # # true value for fixed effects fe_true = numpy . array ( [ a_true , b_true , c_true , phi_true ] ) num_fe = len ( fe_true ) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * b_true / ( n_data - 1 ) social_distance = numpy . zeros ( n_data , dtype = float ) params_true = numpy . zeros (( n_data , 3 ), dtype = float ) alpha_true = numpy . exp ( a_true ) p_true = numpy . exp ( phi_true ) for i in range ( n_data ) : social_distance [ i ] = 0 if i < n_data / 2.0 else 1 beta_true = b_true + c_true * social_distance [ i ] params_true [ i ] = [ alpha_true , beta_true , p_true ] params_true = numpy . transpose ( params_true ) measurement_value = gaussian_cdf ( independent_var , params_true ) measurement_std = n_data * [ 0.1 ] cov_one = n_data * [ 1.0 ] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'cov_one' : cov_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = [ [ 'cov_one' ], [ 'cov_one' , 'social_distance' ], [ 'cov_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = gaussian_cdf col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # fe_init = fe_true / 3.0 re_init = numpy . zeros ( num_fe ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ 0.0 , 0.0 ] ] * num_fe options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , options = options ) fe_estimate = curve_model . result . x [: num_fe ] # ------------------------------------------------------------------------- # check result for i in range ( num_fe ) : rel_error = fe_estimate [ i ] / fe_true [ i ] - 1.0 assert abs ( rel_error ) < rel_tol # print ( 'covariate.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/effects2params/","text":"Map Vector of Fixed and Random Effects to Parameter Matrix Syntax params = curvefit.core.effects2params.effects2params( x, group_sizes, covs, link_fun, var_link_fun, expand=True ) Vector If v is a vector, len(v) is its length as an integer. For non-negative integer i less than len(v) , and v[i] is its i-th element. group_sizes is a vector of positive integers. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The total number of observations is the sum of the group sizes. covs Is a list with length equal to the number of parameters and covs[k] is a two dimensional numpy array with the following contents: -- covs[k].shape[0] is the number of observations -- covs[k].shape[1] is the number of fixed effects corresponding to the k-th parameter. -- covs[k][i, ell] is the covariate value corresponding to the i-th observation and ell-th covariate for the k-th parameter. link_fun The value len(link_fun) is equal to the number of parameters and link_fun[k] is a function with one numpy array argument and result that acts element by element and transforms the k-th parameter. var_link_fun The value len(var_link_fun) is equal to the number of fixed effects and link_fun[j] is a function with one numpy array argument and result that that acts element by element and transforms the j-th fixed effect. The first len(covs[0]) fixed effects correspond to the first parameter, the next len(covs[1]) fixed effects correspond to the second parameter and so on. expand If expand is True ( False ), create parameters for each observation (for each group of observations). x This is a one dimensional numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The i-th sub-vector corresponds to the i-th group of observations. params Let f_j be the vector of fixed effects and r_{i,j} the matrix of random effects corresponding to x . We define the matrix, with row dimension equal the number of groups and column dimension equal the number of fixed effects v_{i,j} = V_j \\left( f_j + r_{i,j} \\right) where V_j is the function var_link_fun[i] . If expand is true (false) i indexes observations (groups). (If expand is true the random effect for a group gets repeated for all the observations in the group.) The return value params is a two dimensional numpy array with params.shape[0] equal to the number of parameters and params.shape[1] equal to the number of observations, if expand is true, number of groups, if expand is false. The value params[k][i] is P_k \\left( \\sum_{j(k)} v_j c_{i,j} \\right) where P_k is the function link_fun[k] , j(k) is the set of fixed effects indices corresponding to the k-th parameter, c_{i,j} is the covariate value corresponding to the j-th fixed effect and the i-th observation, if expand is true, or i-th group, if expand is false. Example effects2params_xam","title":"effects2params"},{"location":"extract_md/effects2params/#map-vector-of-fixed-and-random-effects-to-parameter-matrix","text":"","title":"Map Vector of Fixed and Random Effects to Parameter Matrix"},{"location":"extract_md/effects2params/#syntax","text":"params = curvefit.core.effects2params.effects2params( x, group_sizes, covs, link_fun, var_link_fun, expand=True )","title":"Syntax"},{"location":"extract_md/effects2params/#vector","text":"If v is a vector, len(v) is its length as an integer. For non-negative integer i less than len(v) , and v[i] is its i-th element.","title":"Vector"},{"location":"extract_md/effects2params/#group_sizes","text":"is a vector of positive integers. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The total number of observations is the sum of the group sizes.","title":"group_sizes"},{"location":"extract_md/effects2params/#covs","text":"Is a list with length equal to the number of parameters and covs[k] is a two dimensional numpy array with the following contents: -- covs[k].shape[0] is the number of observations -- covs[k].shape[1] is the number of fixed effects corresponding to the k-th parameter. -- covs[k][i, ell] is the covariate value corresponding to the i-th observation and ell-th covariate for the k-th parameter.","title":"covs"},{"location":"extract_md/effects2params/#link_fun","text":"The value len(link_fun) is equal to the number of parameters and link_fun[k] is a function with one numpy array argument and result that acts element by element and transforms the k-th parameter.","title":"link_fun"},{"location":"extract_md/effects2params/#var_link_fun","text":"The value len(var_link_fun) is equal to the number of fixed effects and link_fun[j] is a function with one numpy array argument and result that that acts element by element and transforms the j-th fixed effect. The first len(covs[0]) fixed effects correspond to the first parameter, the next len(covs[1]) fixed effects correspond to the second parameter and so on.","title":"var_link_fun"},{"location":"extract_md/effects2params/#expand","text":"If expand is True ( False ), create parameters for each observation (for each group of observations).","title":"expand"},{"location":"extract_md/effects2params/#x","text":"This is a one dimensional numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The i-th sub-vector corresponds to the i-th group of observations.","title":"x"},{"location":"extract_md/effects2params/#params","text":"Let f_j be the vector of fixed effects and r_{i,j} the matrix of random effects corresponding to x . We define the matrix, with row dimension equal the number of groups and column dimension equal the number of fixed effects v_{i,j} = V_j \\left( f_j + r_{i,j} \\right) where V_j is the function var_link_fun[i] . If expand is true (false) i indexes observations (groups). (If expand is true the random effect for a group gets repeated for all the observations in the group.) The return value params is a two dimensional numpy array with params.shape[0] equal to the number of parameters and params.shape[1] equal to the number of observations, if expand is true, number of groups, if expand is false. The value params[k][i] is P_k \\left( \\sum_{j(k)} v_j c_{i,j} \\right) where P_k is the function link_fun[k] , j(k) is the set of fixed effects indices corresponding to the k-th parameter, c_{i,j} is the covariate value corresponding to the j-th fixed effect and the i-th observation, if expand is true, or i-th group, if expand is false.","title":"params"},{"location":"extract_md/effects2params/#example","text":"effects2params_xam","title":"Example"},{"location":"extract_md/effects2params_xam/","text":"Example and Test of effects2params} Function Documentation effects2params Example Source Code import sys import numpy import sandbox sandbox . path () import curvefit # ----------------------------------------------------------------------- # Test parameters num_param = 3 num_group = 2 # ----------------------------------------------------------------------- def identity_fun ( x ): return x # num_fe = num_param num_x = ( num_group + 1 ) * num_fe x = numpy . array ( range ( num_x ), dtype = float ) / num_x group_sizes = numpy . arange ( num_group ) * 2 + 1 num_obs = sum ( group_sizes ) covs = list () for k in range ( num_param ) : covs . append ( numpy . ones ( ( num_obs , 1 ), dtype = float ) ) link_fun = [ numpy . exp , identity_fun , numpy . exp ] var_link_fun = num_param * [ identity_fun ] expand = False param = curvefit . core . effects2params . effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand ) # ---------------------------------------------------------------------- # check result eps99 = 99.0 * numpy . finfo ( float ) . eps fe = x [ 0 : num_fe ] re = x [ num_fe :] . reshape ( ( num_group , num_fe ), order = 'C' ) fe_re = fe + re var = numpy . empty ( ( num_group , num_fe ), dtype = float ) for j in range ( num_fe ) : var [:, j ] = var_link_fun [ j ]( fe_re [:, j ] ) check = numpy . empty ( ( num_param , num_group ), dtype = float ) for k in range ( num_param ) : check [ k ,:] = link_fun [ k ]( var [:, k ] * covs [ k ][ 0 ] ) # rel_error = param / check - 1.0 assert ( ( abs ( rel_error ) < eps99 ) . all () ) print ( 'effects2params.py: OK' ) sys . exit ( 0 )","title":"effects2params_xam"},{"location":"extract_md/effects2params_xam/#example-and-test-of-effects2params","text":"","title":"Example and Test of effects2params}"},{"location":"extract_md/effects2params_xam/#function-documentation","text":"effects2params","title":"Function Documentation"},{"location":"extract_md/effects2params_xam/#example-source-code","text":"import sys import numpy import sandbox sandbox . path () import curvefit # ----------------------------------------------------------------------- # Test parameters num_param = 3 num_group = 2 # ----------------------------------------------------------------------- def identity_fun ( x ): return x # num_fe = num_param num_x = ( num_group + 1 ) * num_fe x = numpy . array ( range ( num_x ), dtype = float ) / num_x group_sizes = numpy . arange ( num_group ) * 2 + 1 num_obs = sum ( group_sizes ) covs = list () for k in range ( num_param ) : covs . append ( numpy . ones ( ( num_obs , 1 ), dtype = float ) ) link_fun = [ numpy . exp , identity_fun , numpy . exp ] var_link_fun = num_param * [ identity_fun ] expand = False param = curvefit . core . effects2params . effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand ) # ---------------------------------------------------------------------- # check result eps99 = 99.0 * numpy . finfo ( float ) . eps fe = x [ 0 : num_fe ] re = x [ num_fe :] . reshape ( ( num_group , num_fe ), order = 'C' ) fe_re = fe + re var = numpy . empty ( ( num_group , num_fe ), dtype = float ) for j in range ( num_fe ) : var [:, j ] = var_link_fun [ j ]( fe_re [:, j ] ) check = numpy . empty ( ( num_param , num_group ), dtype = float ) for k in range ( num_param ) : check [ k ,:] = link_fun [ k ]( var [:, k ] * covs [ k ][ 0 ] ) # rel_error = param / check - 1.0 assert ( ( abs ( rel_error ) < eps99 ) . all () ) print ( 'effects2params.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/extract_md.py/","text":"Extracting Markdown Documentation From Source Code Syntax bin/extract_md.py extract_dir The variable extract_dir at top of bin/extract_md.py determines the sub-directory, below the docs directory, where the markdown files will be written. Any files names that end in .md in that directory will be removed at the beginning so that all the files in this directory have been extracted from the current version of the source code. file_list The variable file_list at top of bin/extract_md.py is a list of file names, relative to the top git repository directory, that the markdown files will be extracted from. extra_special_words The variable extra_special_words is a list of extra words that the spell checker will consider correct; see spell checking below. Start Section The start of a markdown section of the input file is indicated by the following text: {begin_markdown section_name } Here section_name is the name of output file corresponding to this section. The possible characters in section_name are A-Z, a-z, 0-9, underbar _ , and dot . mkdocs.yml For each section_name in the documentation there must be a line in the mkdocs.yml file fo the following form: - section_name : ' extract_dir / section_name .md' where there can be any number of spaces around the dash character (-) and the colon character (:). Suspend Markdown It is possible do suspend the markdown output during a section. One begins the suspension with the command { suspend_markdown} and resumes the output with the command { resume_markdown} Note that this will also suspend the markdown processing; e.g., spell checking. Each suspend markdown must have a corresponding resume markdown in same section (between the corresponding begin markdown and end markdown commands). End Section The end of a markdown section of the input file is indicated by the following text: {end_markdown section_name } Here section_name must be the same as in the start of this markdown section. Spell Checking Special words can be added to the correct spelling list for a particular section as follows: { spell_markdown special_1 ... special_n } Here special_1 , ..., special_n are special words that are to be considered valid for this section. In the syntax above they are all on the same line, but they could be on different lines. Each word starts with an upper case letter, a lower case letter, or a back slash. The rest of the characters in a word are lower case letters. The case of the first letter does not matter when checking for special words; e.g., if abcd is special_1 then Abcd will be considered a valid word. The back slash is included at the beginning of a word so that latex commands are considered words. The latex commands corresponding to the letters in the greek alphabet are automatically included. Any latex commands in the extra_special_words are also automatically included. Code Blocks A code block within a markdown section begins and ends with three back quotes. Thus there must be an even number of occurrences of three back quotes. The first three back quotes, for each code block, must have a language name directly after it. The language name must be a sequence of letters; e.g., python . The other characters on the same line as the three back quotes are not included in the markdown output. This enables one to begin or end a comment block without having those characters in the markdown output. Indentation If all of the extracted markdown documentation for a section is indented by the same number of space characters, those space characters are not included in the markdown output. This enables one to indent the markdown so it is grouped with the proper code block in the source. Wish List The following is a wish list for future improvements to extract_md.py : Testing Include an optional command line argument that indicates test mode and runs the extractor through some test files and makes sure the result is correct. Error Messaging Improve the error messaging so that it include the line number of the input file that the error occurred on. Source File Include the path to the source code file that the documentation was extracted from (probably at the end of the section). Double Word Errors Detect double word errors and allow for exceptions by specifying them in a double_word_markdown command. Moving Code Blocks Have a way to include code blocks that are not directly below and in the same file; e.g., one my automatically transfer the prototype for a function, in the same file or a different file, to the documentation for a section.","title":"extract_md.py"},{"location":"extract_md/extract_md.py/#extracting-markdown-documentation-from-source-code","text":"","title":"Extracting Markdown Documentation From Source Code"},{"location":"extract_md/extract_md.py/#syntax","text":"bin/extract_md.py","title":"Syntax"},{"location":"extract_md/extract_md.py/#extract_dir","text":"The variable extract_dir at top of bin/extract_md.py determines the sub-directory, below the docs directory, where the markdown files will be written. Any files names that end in .md in that directory will be removed at the beginning so that all the files in this directory have been extracted from the current version of the source code.","title":"extract_dir"},{"location":"extract_md/extract_md.py/#file_list","text":"The variable file_list at top of bin/extract_md.py is a list of file names, relative to the top git repository directory, that the markdown files will be extracted from.","title":"file_list"},{"location":"extract_md/extract_md.py/#extra_special_words","text":"The variable extra_special_words is a list of extra words that the spell checker will consider correct; see spell checking below.","title":"extra_special_words"},{"location":"extract_md/extract_md.py/#start-section","text":"The start of a markdown section of the input file is indicated by the following text: {begin_markdown section_name } Here section_name is the name of output file corresponding to this section. The possible characters in section_name are A-Z, a-z, 0-9, underbar _ , and dot .","title":"Start Section"},{"location":"extract_md/extract_md.py/#mkdocsyml","text":"For each section_name in the documentation there must be a line in the mkdocs.yml file fo the following form: - section_name : ' extract_dir / section_name .md' where there can be any number of spaces around the dash character (-) and the colon character (:).","title":"mkdocs.yml"},{"location":"extract_md/extract_md.py/#suspend-markdown","text":"It is possible do suspend the markdown output during a section. One begins the suspension with the command { suspend_markdown} and resumes the output with the command { resume_markdown} Note that this will also suspend the markdown processing; e.g., spell checking. Each suspend markdown must have a corresponding resume markdown in same section (between the corresponding begin markdown and end markdown commands).","title":"Suspend Markdown"},{"location":"extract_md/extract_md.py/#end-section","text":"The end of a markdown section of the input file is indicated by the following text: {end_markdown section_name } Here section_name must be the same as in the start of this markdown section.","title":"End Section"},{"location":"extract_md/extract_md.py/#spell-checking","text":"Special words can be added to the correct spelling list for a particular section as follows: { spell_markdown special_1 ... special_n } Here special_1 , ..., special_n are special words that are to be considered valid for this section. In the syntax above they are all on the same line, but they could be on different lines. Each word starts with an upper case letter, a lower case letter, or a back slash. The rest of the characters in a word are lower case letters. The case of the first letter does not matter when checking for special words; e.g., if abcd is special_1 then Abcd will be considered a valid word. The back slash is included at the beginning of a word so that latex commands are considered words. The latex commands corresponding to the letters in the greek alphabet are automatically included. Any latex commands in the extra_special_words are also automatically included.","title":"Spell Checking"},{"location":"extract_md/extract_md.py/#code-blocks","text":"A code block within a markdown section begins and ends with three back quotes. Thus there must be an even number of occurrences of three back quotes. The first three back quotes, for each code block, must have a language name directly after it. The language name must be a sequence of letters; e.g., python . The other characters on the same line as the three back quotes are not included in the markdown output. This enables one to begin or end a comment block without having those characters in the markdown output.","title":"Code Blocks"},{"location":"extract_md/extract_md.py/#indentation","text":"If all of the extracted markdown documentation for a section is indented by the same number of space characters, those space characters are not included in the markdown output. This enables one to indent the markdown so it is grouped with the proper code block in the source.","title":"Indentation"},{"location":"extract_md/extract_md.py/#wish-list","text":"The following is a wish list for future improvements to extract_md.py :","title":"Wish List"},{"location":"extract_md/extract_md.py/#testing","text":"Include an optional command line argument that indicates test mode and runs the extractor through some test files and makes sure the result is correct.","title":"Testing"},{"location":"extract_md/extract_md.py/#error-messaging","text":"Improve the error messaging so that it include the line number of the input file that the error occurred on.","title":"Error Messaging"},{"location":"extract_md/extract_md.py/#source-file","text":"Include the path to the source code file that the documentation was extracted from (probably at the end of the section).","title":"Source File"},{"location":"extract_md/extract_md.py/#double-word-errors","text":"Detect double word errors and allow for exceptions by specifying them in a double_word_markdown command.","title":"Double Word Errors"},{"location":"extract_md/extract_md.py/#moving-code-blocks","text":"Have a way to include code blocks that are not directly below and in the same file; e.g., one my automatically transfer the prototype for a function, in the same file or a different file, to the documentation for a section.","title":"Moving Code Blocks"},{"location":"extract_md/get_cppad_py.py/","text":"Download and Install cppad_py Syntax `bin/get_cppad_py.py [test] [debug] [prefix] Command Line Arguments The order of the command line arguments test , debug , and prefix does not matter test If this argument is present it must be test=True . In this case cppad_py will be tested before the install. debug If this argument is present it must be debug=True . In this case the debug version of cppad_py will be built (possible tested) and installed. prefix If this argument is present it must be prefix= path . In this case the path will be the prefix where cppad_py is installed. Otherwise the standard setup.py path is used.","title":"get_cppad_py.py"},{"location":"extract_md/get_cppad_py.py/#download-and-install-cppad_py","text":"","title":"Download and Install cppad_py"},{"location":"extract_md/get_cppad_py.py/#syntax","text":"`bin/get_cppad_py.py [test] [debug] [prefix]","title":"Syntax"},{"location":"extract_md/get_cppad_py.py/#command-line-arguments","text":"The order of the command line arguments test , debug , and prefix does not matter","title":"Command Line Arguments"},{"location":"extract_md/get_cppad_py.py/#test","text":"If this argument is present it must be test=True . In this case cppad_py will be tested before the install.","title":"test"},{"location":"extract_md/get_cppad_py.py/#debug","text":"If this argument is present it must be debug=True . In this case the debug version of cppad_py will be built (possible tested) and installed.","title":"debug"},{"location":"extract_md/get_cppad_py.py/#prefix","text":"If this argument is present it must be prefix= path . In this case the path will be the prefix where cppad_py is installed. Otherwise the standard setup.py path is used.","title":"prefix"},{"location":"extract_md/get_started_xam/","text":"Getting Started Using CurveFit Generalized Logistic Model The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters. Fixed Effects We use the notation a , b and \\phi for the fixed effect corresponding to the parameters \\alpha , \\beta , p respectively. For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b \\\\ p & = \\exp( \\phi ) \\end{aligned} The fixed effects are initialized to be their true values divided by three. Random effects For this example the random effects are constrained to be zero. Covariates This example data set has two covariates, the constant one and a social distance measure. While the social distance is in the data set, it is not used. Simulated data Problem Settings The following settings are used to simulate the data and check that the solution is correct: n_data = 21 # number simulated measurements to generate beta_true = 20.0 # max death rate at 20 days alpha_true = 2.0 / beta_true # alpha_true * beta_true = 2.0 p_true = 0.1 # maximum cumulative death fraction rel_tol = 1e-5 # relative tolerance used to check optimal solution Time Grid A grid of n_data points in time, t_i , where t_i = \\beta_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is \\beta . Measurement values We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , \\beta_T , p_T ) Note that when we do the fitting, we model each data point as having noise. Example Source Code # ------------------------------------------------------------------------- import scipy import sys import pandas import numpy import sandbox sandbox . path () import curvefit # # for this model number of parameters is same as number of fixed effects num_params = 3 num_fe = 3 # # f(t, alpha, beta, p) def expit ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # # identity function def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # inverse of function used for alpha, p def log_fun ( x ) : return numpy . log ( x ) # # params_true params_true = numpy . array ( [ alpha_true , beta_true , p_true ] ) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * beta_true / ( n_data - 1 ) measurement_value = expit ( independent_var , params_true ) measurement_std = n_data * [ 0.1 ] constant_one = n_data * [ 1.0 ] social_distance = [ 0.0 if i < n_data / 2 else 1.0 for i in range ( n_data ) ] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = num_params * [ [ 'constant_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = expit col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # # initialize fixed effects so correspond to true parameters divided by three inv_link_fun = [ log_fun , identity_fun , log_fun ] fe_init = numpy . zeros ( num_fe ) for i in range ( num_fe ) : fe_init [ i ] = inv_link_fun [ i ]( params_true [ i ]) / 3.0 # re_init = numpy . zeros ( num_fe ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ 0.0 , 0.0 ] ] * num_fe options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , options = options ) params_estimate = curve_model . params fe_estimate = curve_model . result . x [: num_fe ] # ------------------------------------------------------------------------- # check relation between fixed effects and parameters for i in range ( num_fe ) : rel_error = params_estimate [ i ] / link_fun [ i ]( fe_estimate [ i ] ) - 1.0 assert abs ( rel_error ) < 10.0 * numpy . finfo ( float ) . eps # # check optimal parameters for i in range ( num_params ) : rel_error = params_estimate [ i ] / params_true [ i ] - 1.0 assert abs ( rel_error ) < rel_tol # print ( 'get_started.py: OK' ) sys . exit ( 0 )","title":"get_started_xam"},{"location":"extract_md/get_started_xam/#getting-started-using-curvefit","text":"","title":"Getting Started Using CurveFit"},{"location":"extract_md/get_started_xam/#generalized-logistic-model","text":"The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters.","title":"Generalized Logistic Model"},{"location":"extract_md/get_started_xam/#fixed-effects","text":"We use the notation a , b and \\phi for the fixed effect corresponding to the parameters \\alpha , \\beta , p respectively. For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b \\\\ p & = \\exp( \\phi ) \\end{aligned} The fixed effects are initialized to be their true values divided by three.","title":"Fixed Effects"},{"location":"extract_md/get_started_xam/#random-effects","text":"For this example the random effects are constrained to be zero.","title":"Random effects"},{"location":"extract_md/get_started_xam/#covariates","text":"This example data set has two covariates, the constant one and a social distance measure. While the social distance is in the data set, it is not used.","title":"Covariates"},{"location":"extract_md/get_started_xam/#simulated-data","text":"","title":"Simulated data"},{"location":"extract_md/get_started_xam/#problem-settings","text":"The following settings are used to simulate the data and check that the solution is correct: n_data = 21 # number simulated measurements to generate beta_true = 20.0 # max death rate at 20 days alpha_true = 2.0 / beta_true # alpha_true * beta_true = 2.0 p_true = 0.1 # maximum cumulative death fraction rel_tol = 1e-5 # relative tolerance used to check optimal solution","title":"Problem Settings"},{"location":"extract_md/get_started_xam/#time-grid","text":"A grid of n_data points in time, t_i , where t_i = \\beta_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is \\beta .","title":"Time Grid"},{"location":"extract_md/get_started_xam/#measurement-values","text":"We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , \\beta_T , p_T ) Note that when we do the fitting, we model each data point as having noise.","title":"Measurement values"},{"location":"extract_md/get_started_xam/#example-source-code","text":"# ------------------------------------------------------------------------- import scipy import sys import pandas import numpy import sandbox sandbox . path () import curvefit # # for this model number of parameters is same as number of fixed effects num_params = 3 num_fe = 3 # # f(t, alpha, beta, p) def expit ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # # identity function def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # inverse of function used for alpha, p def log_fun ( x ) : return numpy . log ( x ) # # params_true params_true = numpy . array ( [ alpha_true , beta_true , p_true ] ) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * beta_true / ( n_data - 1 ) measurement_value = expit ( independent_var , params_true ) measurement_std = n_data * [ 0.1 ] constant_one = n_data * [ 1.0 ] social_distance = [ 0.0 if i < n_data / 2 else 1.0 for i in range ( n_data ) ] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = num_params * [ [ 'constant_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = expit col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # # initialize fixed effects so correspond to true parameters divided by three inv_link_fun = [ log_fun , identity_fun , log_fun ] fe_init = numpy . zeros ( num_fe ) for i in range ( num_fe ) : fe_init [ i ] = inv_link_fun [ i ]( params_true [ i ]) / 3.0 # re_init = numpy . zeros ( num_fe ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ 0.0 , 0.0 ] ] * num_fe options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , options = options ) params_estimate = curve_model . params fe_estimate = curve_model . result . x [: num_fe ] # ------------------------------------------------------------------------- # check relation between fixed effects and parameters for i in range ( num_fe ) : rel_error = params_estimate [ i ] / link_fun [ i ]( fe_estimate [ i ] ) - 1.0 assert abs ( rel_error ) < 10.0 * numpy . finfo ( float ) . eps # # check optimal parameters for i in range ( num_params ) : rel_error = params_estimate [ i ] / params_true [ i ] - 1.0 assert abs ( rel_error ) < rel_tol # print ( 'get_started.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/loss_xam/","text":"Example and Test of Loss Functions Function Documentation st_loss , normal_loss Example Source Code import sys import numpy import cppad_py import sandbox sandbox . path () import curvefit # ---------------------------------------------------------------------------- # Loss Functions # ---------------------------------------------------------------------------- eps99 = 99.0 * numpy . finfo ( float ) . eps # # test values for t, param r = numpy . array ( [ 1 , 2 , 3 ], dtype = float ) nu = numpy . array ( [ 3 , 2 , 1 ], dtype = float ) # ----------------------------------------------------------------------- # f(t) = st_loss ar = cppad_py . independent ( r ) aloss = curvefit . core . functions . st_loss ( ar , nu ) ay = numpy . array ( [ aloss ] ) f = cppad_py . d_fun ( ar , ay ) # y = f . forward ( 0 , r ) check = curvefit . core . functions . st_loss ( r , nu ) rel_error = y [ 0 ] / check - 1.0 assert abs ( rel_error ) < eps99 # ----------------------------------------------------------------------- # f(t) = normal_loss ar = cppad_py . independent ( r ) aloss = curvefit . core . functions . normal_loss ( ar ) ay = numpy . array ( [ aloss ] ) f = cppad_py . d_fun ( ar , ay ) # y = f . forward ( 0 , r ) check = curvefit . core . functions . normal_loss ( r ) rel_error = y [ 0 ] / check - 1.0 assert abs ( rel_error ) < eps99 # ----------------------------------------------------------------------- print ( 'loss.py: OK' ) sys . exit ( 0 )","title":"loss_xam"},{"location":"extract_md/loss_xam/#example-and-test-of-loss-functions","text":"","title":"Example and Test of Loss Functions"},{"location":"extract_md/loss_xam/#function-documentation","text":"st_loss , normal_loss","title":"Function Documentation"},{"location":"extract_md/loss_xam/#example-source-code","text":"import sys import numpy import cppad_py import sandbox sandbox . path () import curvefit # ---------------------------------------------------------------------------- # Loss Functions # ---------------------------------------------------------------------------- eps99 = 99.0 * numpy . finfo ( float ) . eps # # test values for t, param r = numpy . array ( [ 1 , 2 , 3 ], dtype = float ) nu = numpy . array ( [ 3 , 2 , 1 ], dtype = float ) # ----------------------------------------------------------------------- # f(t) = st_loss ar = cppad_py . independent ( r ) aloss = curvefit . core . functions . st_loss ( ar , nu ) ay = numpy . array ( [ aloss ] ) f = cppad_py . d_fun ( ar , ay ) # y = f . forward ( 0 , r ) check = curvefit . core . functions . st_loss ( r , nu ) rel_error = y [ 0 ] / check - 1.0 assert abs ( rel_error ) < eps99 # ----------------------------------------------------------------------- # f(t) = normal_loss ar = cppad_py . independent ( r ) aloss = curvefit . core . functions . normal_loss ( ar ) ay = numpy . array ( [ aloss ] ) f = cppad_py . d_fun ( ar , ay ) # y = f . forward ( 0 , r ) check = curvefit . core . functions . normal_loss ( r ) rel_error = y [ 0 ] / check - 1.0 assert abs ( rel_error ) < eps99 # ----------------------------------------------------------------------- print ( 'loss.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/normal_loss/","text":"Gaussian Loss Function Syntax loss = curvefit.core.loss_fun.normal_loss(r) r is a numpy vector of normalized residual values. We use n for the length of the vector. The elements of this vector can be float or a_double values. Distribution The Gaussian distribution is f(x) = \\exp \\left[ - (1/2) ( x - \\mu )^2 / \\sigma^2 \\right] / \\left( \\sigma \\sqrt{ 2 \\pi } \\right) where \\mu is the mean and \\sigma is the standard deviation. Negative log Taking the negative log of the distribution function we get - \\log [ f(x) ] = (1/2) ( x - \\mu )^2 / \\sigma^2 + c where c is constant w.r.t. x . loss The return value loss is a scalar equal to \\frac{1}{2} \\sum_{i=1}^{n-1} r_i^2 where r_i = ( x_i - \\mu)) / \\sigma . Example loss_xam","title":"normal_loss"},{"location":"extract_md/normal_loss/#gaussian-loss-function","text":"","title":"Gaussian Loss Function"},{"location":"extract_md/normal_loss/#syntax","text":"loss = curvefit.core.loss_fun.normal_loss(r)","title":"Syntax"},{"location":"extract_md/normal_loss/#r","text":"is a numpy vector of normalized residual values. We use n for the length of the vector. The elements of this vector can be float or a_double values.","title":"r"},{"location":"extract_md/normal_loss/#distribution","text":"The Gaussian distribution is f(x) = \\exp \\left[ - (1/2) ( x - \\mu )^2 / \\sigma^2 \\right] / \\left( \\sigma \\sqrt{ 2 \\pi } \\right) where \\mu is the mean and \\sigma is the standard deviation.","title":"Distribution"},{"location":"extract_md/normal_loss/#negative-log","text":"Taking the negative log of the distribution function we get - \\log [ f(x) ] = (1/2) ( x - \\mu )^2 / \\sigma^2 + c where c is constant w.r.t. x .","title":"Negative log"},{"location":"extract_md/normal_loss/#loss","text":"The return value loss is a scalar equal to \\frac{1}{2} \\sum_{i=1}^{n-1} r_i^2 where r_i = ( x_i - \\mu)) / \\sigma .","title":"loss"},{"location":"extract_md/normal_loss/#example","text":"loss_xam","title":"Example"},{"location":"extract_md/objective_fun/","text":"Curve Fitting Objective Function of Fixed and Random Effects Syntax obj_val = curvefit.core.objective_fun.objective_fun( x, t, obs, obs_se, covs, group_sizes, model_fun, loss_fun, link_fun, var_link_fun, fe_gprior, re_gprior, param_gprior, ) : Notation num_obs = len(obs) is the number of observations (measurements) num_param = len(covs) is the number of parameters in the model. num_fe = fe_gprior.shape[0] is the number of fixed effects. num_group = len(group_sizes) is the number of groups params = `effects2params(x, group_sizes, covs, link_fun, var_link_fun) is a num_param by num_obs numpy array containing the parameters corresponding to each observation; see effects2params . A vector is either a list or a one dimension numpy.array . x is a one dimension numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The i-th sub-vector corresponds to the i-th group of observations. t is a one dimension numpy array with length num_obs containing the value of the independent variable corresponding to each observation. obs is a one dimension numpy array with length num_obs containing the observations (i.e. measurements). obs_se is a one dimension numpy array with length num_obs containing the standard deviation for the corresponding observation. covs Is a list with length equal to the number of parameters and covs[k] is a two dimension numpy array with the following contents: -- covs[k].shape[0] is the number of observations -- covs[k].shape[1] is the number of fixed effects corresponding to the k-th parameter. -- covs[k][i, ell] is the covariate value corresponding to the i-th observation and ell-th covariate for the k-th parameter. group_sizes The observations are divided into groups. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The the sum of the group sizes is equal to num_obs . model_fun This vector valued function vector values maps parameter values, params returned by effects2params , to the model for the corresponding noiseless observations. The observation residual vector has length num_obs and is given by obs_res = ( obs - model_fun ( t , params )) / obs_se loss_fun This scalar value function maps the observation residual vector to the corresponding contribution to the objective function. For example, if loss_fun corresponds to a Gaussian likelihood, it is equal to gaussian_loss ( obs_res ) = 0.5 * sum ( obs_res * obs_res ) link_fun, var_link_fun are used to compute params ; see Notation fe_gprior is an num_fe by two numpy array. The value fe_gprior[j][0] is the prior mean for the j-th fixed effect and fe_gprior[j][1] is its standard deviation. If fe is the fixed effect sub-vector of x , the prior residual for the fixed effects is fe_res = ( fe . T - fe_gprior [:, 0 ] ) / fe_gprior [:, 1 ] were fe.T denotes the transpose of fe . re_gprior is an num_fe by num_groups by by two numpy array, re_gprior[j,i,0] ( re_gprior[j,i,1] ) is the mean (standard deviation) for the random effect corresponding to the j-th fixed effect and the i-th group. If re is the matrix of random effect corresponding to x , the prior residual for the random effects is re_res = ( re . T - re_gprior [:,:, 0 ] ) / re_gprior [:,:, 1 ] param_gprior is a list with three elements. The first element is a function of the params and its result is a numpy array. We use the notation range_gprior = param_gprior [ 0 ]( params ) There is a subtlety here, column dimension of the params above is num_groups (not num_obs). The value param_gprior[1][0] ( param_gprior[1][1] ) is a numpy array corresponding to the mean (standard deviation) for range_gprior*. The prior residual for the parameters is param_res = ( range_gprior - param_gprior [ 1 ][[ 0 ]]) / param_gprior [ 1 ][ 1 ] obj_val The return val is a float equal to the objective function obj_val = loss_fun ( obs_res ) + gaussian_loss ( fe_res ) + gaussian_loss ( re_res ) + gaussian_loss ( param_res ) Example objective_fun_xam","title":"objective_fun"},{"location":"extract_md/objective_fun/#curve-fitting-objective-function-of-fixed-and-random-effects","text":"","title":"Curve Fitting Objective Function of Fixed and Random Effects"},{"location":"extract_md/objective_fun/#syntax","text":"obj_val = curvefit.core.objective_fun.objective_fun( x, t, obs, obs_se, covs, group_sizes, model_fun, loss_fun, link_fun, var_link_fun, fe_gprior, re_gprior, param_gprior, ) :","title":"Syntax"},{"location":"extract_md/objective_fun/#notation","text":"num_obs = len(obs) is the number of observations (measurements) num_param = len(covs) is the number of parameters in the model. num_fe = fe_gprior.shape[0] is the number of fixed effects. num_group = len(group_sizes) is the number of groups params = `effects2params(x, group_sizes, covs, link_fun, var_link_fun) is a num_param by num_obs numpy array containing the parameters corresponding to each observation; see effects2params . A vector is either a list or a one dimension numpy.array .","title":"Notation"},{"location":"extract_md/objective_fun/#x","text":"is a one dimension numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The i-th sub-vector corresponds to the i-th group of observations.","title":"x"},{"location":"extract_md/objective_fun/#t","text":"is a one dimension numpy array with length num_obs containing the value of the independent variable corresponding to each observation.","title":"t"},{"location":"extract_md/objective_fun/#obs","text":"is a one dimension numpy array with length num_obs containing the observations (i.e. measurements).","title":"obs"},{"location":"extract_md/objective_fun/#obs_se","text":"is a one dimension numpy array with length num_obs containing the standard deviation for the corresponding observation.","title":"obs_se"},{"location":"extract_md/objective_fun/#covs","text":"Is a list with length equal to the number of parameters and covs[k] is a two dimension numpy array with the following contents: -- covs[k].shape[0] is the number of observations -- covs[k].shape[1] is the number of fixed effects corresponding to the k-th parameter. -- covs[k][i, ell] is the covariate value corresponding to the i-th observation and ell-th covariate for the k-th parameter.","title":"covs"},{"location":"extract_md/objective_fun/#group_sizes","text":"The observations are divided into groups. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The the sum of the group sizes is equal to num_obs .","title":"group_sizes"},{"location":"extract_md/objective_fun/#model_fun","text":"This vector valued function vector values maps parameter values, params returned by effects2params , to the model for the corresponding noiseless observations. The observation residual vector has length num_obs and is given by obs_res = ( obs - model_fun ( t , params )) / obs_se","title":"model_fun"},{"location":"extract_md/objective_fun/#loss_fun","text":"This scalar value function maps the observation residual vector to the corresponding contribution to the objective function. For example, if loss_fun corresponds to a Gaussian likelihood, it is equal to gaussian_loss ( obs_res ) = 0.5 * sum ( obs_res * obs_res )","title":"loss_fun"},{"location":"extract_md/objective_fun/#link_fun-var_link_fun","text":"are used to compute params ; see Notation","title":"link_fun, var_link_fun"},{"location":"extract_md/objective_fun/#fe_gprior","text":"is an num_fe by two numpy array. The value fe_gprior[j][0] is the prior mean for the j-th fixed effect and fe_gprior[j][1] is its standard deviation. If fe is the fixed effect sub-vector of x , the prior residual for the fixed effects is fe_res = ( fe . T - fe_gprior [:, 0 ] ) / fe_gprior [:, 1 ] were fe.T denotes the transpose of fe .","title":"fe_gprior"},{"location":"extract_md/objective_fun/#re_gprior","text":"is an num_fe by num_groups by by two numpy array, re_gprior[j,i,0] ( re_gprior[j,i,1] ) is the mean (standard deviation) for the random effect corresponding to the j-th fixed effect and the i-th group. If re is the matrix of random effect corresponding to x , the prior residual for the random effects is re_res = ( re . T - re_gprior [:,:, 0 ] ) / re_gprior [:,:, 1 ]","title":"re_gprior"},{"location":"extract_md/objective_fun/#param_gprior","text":"is a list with three elements. The first element is a function of the params and its result is a numpy array. We use the notation range_gprior = param_gprior [ 0 ]( params ) There is a subtlety here, column dimension of the params above is num_groups (not num_obs). The value param_gprior[1][0] ( param_gprior[1][1] ) is a numpy array corresponding to the mean (standard deviation) for range_gprior*. The prior residual for the parameters is param_res = ( range_gprior - param_gprior [ 1 ][[ 0 ]]) / param_gprior [ 1 ][ 1 ]","title":"param_gprior"},{"location":"extract_md/objective_fun/#obj_val","text":"The return val is a float equal to the objective function obj_val = loss_fun ( obs_res ) + gaussian_loss ( fe_res ) + gaussian_loss ( re_res ) + gaussian_loss ( param_res )","title":"obj_val"},{"location":"extract_md/objective_fun/#example","text":"objective_fun_xam","title":"Example"},{"location":"extract_md/objective_fun_xam/","text":"Example and Test of objective_fun Function Documentation [objective_fun][objective_fun.md] Example Source Code import sys import numpy import sandbox sandbox . path () import curvefit # ----------------------------------------------------------------------- # Test parameters num_param = 3 num_group = 2 # ----------------------------------------------------------------------- # arguments to objective_fun def identity_fun ( x ): return x def gaussian_loss ( x ) : return numpy . sum ( x * x ) / 2.0 # num_fe = num_param num_re = num_group * num_fe fe = numpy . array ( range ( num_fe ), dtype = float ) / num_fe re = numpy . array ( range ( num_re ), dtype = float ) / num_re group_sizes = ( numpy . arange ( num_group ) + 1 ) * 2 # x = numpy . concatenate ( ( fe , re ) ) num_obs = sum ( group_sizes ) t = numpy . arange ( num_obs , dtype = float ) obs = numpy . arange ( num_obs , dtype = float ) / num_obs obs_se = ( obs + 1.0 ) / 10.0 # covs covs = list () for k in range ( num_param ) : covs . append ( numpy . ones ( ( num_obs , 1 ), dtype = float ) ) # model_fun = curvefit . core . functions . gaussian_cdf loss_fun = gaussian_loss link_fun = [ numpy . exp , identity_fun , numpy . exp ] var_link_fun = num_param * [ identity_fun ] # fe_gprior fe_gprior = numpy . empty ( ( num_fe , 2 ), dtype = float ) for j in range ( num_fe ) : fe_gprior [ j , 0 ] = j / ( 2.0 * num_fe ) fe_gprior [:, 1 ] = 1.0 + fe_gprior [:, 0 ] * 1.2 # # re_gprior, param_gprior re_gprior = numpy . empty ( ( num_fe , num_group , 2 ), dtype = float ) param_gprior_mean = numpy . empty ( ( num_param , num_group ), dtype = float ) for i in range ( num_group ) : for j in range ( num_fe ) : # the matrix re_gprior[:,:,0] is the transposed from the order in re re_gprior [ j , i , 0 ] = ( i + j ) / ( 2.0 * ( num_fe + num_re )) k = j param_gprior_mean [ k , i ] = ( i + k ) / ( 3.0 * ( num_fe + num_re )) re_gprior [:, :, 1 ] = ( 1.0 + re_gprior [:, :, 0 ] / 3.0 ) param_gprior_std = ( 1.0 + param_gprior_mean / 2.0 ) param_gprior_fun = identity_fun param_gprior = [ param_gprior_fun , param_gprior_mean , param_gprior_std ] # ----------------------------------------------------------------------- # call to objective_fun obj_val = curvefit . core . objective_fun . objective_fun ( x , t , obs , obs_se , covs , group_sizes , model_fun , loss_fun , link_fun , var_link_fun , fe_gprior , re_gprior , param_gprior ) # ----------------------------------------------------------------------- # check objective_fun return value expand = True effects2params = curvefit . core . effects2params . effects2params unzip_x = curvefit . core . effects2params . unzip_x param = effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand ) fe , re = unzip_x ( x , num_group , num_fe ) obs_res = ( obs - model_fun ( t , param )) / obs_se fe_res = ( fe . T - fe_gprior [:, 0 ]) / fe_gprior [:, 1 ] re_res = ( re . T - re_gprior [:,:, 0 ]) / re_gprior [:,:, 1 ] expand = False param = effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand ) range_gprior = param_gprior [ 0 ]( param ) param_res = ( range_gprior - param_gprior [ 1 ][ 0 ]) / param_gprior [ 1 ][ 1 ] check = loss_fun ( obs_res ) + gaussian_loss ( fe_res ) check += gaussian_loss ( re_res ) + gaussian_loss ( param_res ) rel_error = obj_val / check - 1.0 eps99 = 99.0 * numpy . finfo ( float ) . eps assert abs ( rel_error ) < eps99 # print ( 'objective_fun.py: OK' ) sys . exit ( 0 )","title":"objective_fun_xam"},{"location":"extract_md/objective_fun_xam/#example-and-test-of-objective_fun","text":"","title":"Example and Test of objective_fun"},{"location":"extract_md/objective_fun_xam/#function-documentation","text":"[objective_fun][objective_fun.md]","title":"Function Documentation"},{"location":"extract_md/objective_fun_xam/#example-source-code","text":"import sys import numpy import sandbox sandbox . path () import curvefit # ----------------------------------------------------------------------- # Test parameters num_param = 3 num_group = 2 # ----------------------------------------------------------------------- # arguments to objective_fun def identity_fun ( x ): return x def gaussian_loss ( x ) : return numpy . sum ( x * x ) / 2.0 # num_fe = num_param num_re = num_group * num_fe fe = numpy . array ( range ( num_fe ), dtype = float ) / num_fe re = numpy . array ( range ( num_re ), dtype = float ) / num_re group_sizes = ( numpy . arange ( num_group ) + 1 ) * 2 # x = numpy . concatenate ( ( fe , re ) ) num_obs = sum ( group_sizes ) t = numpy . arange ( num_obs , dtype = float ) obs = numpy . arange ( num_obs , dtype = float ) / num_obs obs_se = ( obs + 1.0 ) / 10.0 # covs covs = list () for k in range ( num_param ) : covs . append ( numpy . ones ( ( num_obs , 1 ), dtype = float ) ) # model_fun = curvefit . core . functions . gaussian_cdf loss_fun = gaussian_loss link_fun = [ numpy . exp , identity_fun , numpy . exp ] var_link_fun = num_param * [ identity_fun ] # fe_gprior fe_gprior = numpy . empty ( ( num_fe , 2 ), dtype = float ) for j in range ( num_fe ) : fe_gprior [ j , 0 ] = j / ( 2.0 * num_fe ) fe_gprior [:, 1 ] = 1.0 + fe_gprior [:, 0 ] * 1.2 # # re_gprior, param_gprior re_gprior = numpy . empty ( ( num_fe , num_group , 2 ), dtype = float ) param_gprior_mean = numpy . empty ( ( num_param , num_group ), dtype = float ) for i in range ( num_group ) : for j in range ( num_fe ) : # the matrix re_gprior[:,:,0] is the transposed from the order in re re_gprior [ j , i , 0 ] = ( i + j ) / ( 2.0 * ( num_fe + num_re )) k = j param_gprior_mean [ k , i ] = ( i + k ) / ( 3.0 * ( num_fe + num_re )) re_gprior [:, :, 1 ] = ( 1.0 + re_gprior [:, :, 0 ] / 3.0 ) param_gprior_std = ( 1.0 + param_gprior_mean / 2.0 ) param_gprior_fun = identity_fun param_gprior = [ param_gprior_fun , param_gprior_mean , param_gprior_std ] # ----------------------------------------------------------------------- # call to objective_fun obj_val = curvefit . core . objective_fun . objective_fun ( x , t , obs , obs_se , covs , group_sizes , model_fun , loss_fun , link_fun , var_link_fun , fe_gprior , re_gprior , param_gprior ) # ----------------------------------------------------------------------- # check objective_fun return value expand = True effects2params = curvefit . core . effects2params . effects2params unzip_x = curvefit . core . effects2params . unzip_x param = effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand ) fe , re = unzip_x ( x , num_group , num_fe ) obs_res = ( obs - model_fun ( t , param )) / obs_se fe_res = ( fe . T - fe_gprior [:, 0 ]) / fe_gprior [:, 1 ] re_res = ( re . T - re_gprior [:,:, 0 ]) / re_gprior [:,:, 1 ] expand = False param = effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand ) range_gprior = param_gprior [ 0 ]( param ) param_res = ( range_gprior - param_gprior [ 1 ][ 0 ]) / param_gprior [ 1 ][ 1 ] check = loss_fun ( obs_res ) + gaussian_loss ( fe_res ) check += gaussian_loss ( re_res ) + gaussian_loss ( param_res ) rel_error = obj_val / check - 1.0 eps99 = 99.0 * numpy . finfo ( float ) . eps assert abs ( rel_error ) < eps99 # print ( 'objective_fun.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/param_time_fun/","text":"Predefined Parametric Functions of Time head Syntax result = curvefit.core.functions.fun(t, params) t This is a list or one dimensional numpy.array . params This is either a list , or numpy.array with one or two dimensions. In any case, len(params) == 3 . If params is a two dimensional array, params.shape[1] == len(t) . We use the notation below for the values in params : Notation Definition \\alpha params[0] \\beta params[1] p params[2] fun The possible values for fun are listed in the subheadings below: expit This is the generalized logistic function which is defined by \\mbox{expit} ( t , \\alpha , \\beta , p ) = \\frac{p}{ 1.0 + \\exp [ - \\alpha ( t - \\beta ) ] } ln_expit This is the log of the generalized logistic function which is defined by \\mbox{ln_expit} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{expit} ( t , \\alpha , \\beta , p ) gaussian_cdf This is the generalized Gaussian cumulative distribution function which is defined by \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\frac{p}{2} \\left[ 1.0 + \\frac{2}{\\pi} \\int_0^{\\alpha(t-\\beta)} \\exp ( - \\tau^2 ) d \\tau \\right] ln_gaussian_cdf This is the log of the generalized Gaussian cumulative distribution function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) gaussian_pdf This is the derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) ln_gaussian_pdf This is the log of the derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) dgaussian_pdf This is the second derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{dgaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) result The result is a list or one dimensional numpy.array with len(result) == len(t) . If params is a list or one dimensional array result [ i ] = fun ( t [ i ], alpha , beta , p ) If params is a two dimensional array result [ i ] = fun ( t [ i ], alpha [ i ], beta [ i ], p [ i ]) Example param_time_fun_xam","title":"param_time_fun"},{"location":"extract_md/param_time_fun/#predefined-parametric-functions-of-time","text":"","title":"Predefined Parametric Functions of Time"},{"location":"extract_md/param_time_fun/#head-syntax","text":"result = curvefit.core.functions.fun(t, params)","title":"head Syntax"},{"location":"extract_md/param_time_fun/#t","text":"This is a list or one dimensional numpy.array .","title":"t"},{"location":"extract_md/param_time_fun/#params","text":"This is either a list , or numpy.array with one or two dimensions. In any case, len(params) == 3 . If params is a two dimensional array, params.shape[1] == len(t) . We use the notation below for the values in params : Notation Definition \\alpha params[0] \\beta params[1] p params[2]","title":"params"},{"location":"extract_md/param_time_fun/#fun","text":"The possible values for fun are listed in the subheadings below:","title":"fun"},{"location":"extract_md/param_time_fun/#expit","text":"This is the generalized logistic function which is defined by \\mbox{expit} ( t , \\alpha , \\beta , p ) = \\frac{p}{ 1.0 + \\exp [ - \\alpha ( t - \\beta ) ] }","title":"expit"},{"location":"extract_md/param_time_fun/#ln_expit","text":"This is the log of the generalized logistic function which is defined by \\mbox{ln_expit} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{expit} ( t , \\alpha , \\beta , p )","title":"ln_expit"},{"location":"extract_md/param_time_fun/#gaussian_cdf","text":"This is the generalized Gaussian cumulative distribution function which is defined by \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\frac{p}{2} \\left[ 1.0 + \\frac{2}{\\pi} \\int_0^{\\alpha(t-\\beta)} \\exp ( - \\tau^2 ) d \\tau \\right]","title":"gaussian_cdf"},{"location":"extract_md/param_time_fun/#ln_gaussian_cdf","text":"This is the log of the generalized Gaussian cumulative distribution function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p )","title":"ln_gaussian_cdf"},{"location":"extract_md/param_time_fun/#gaussian_pdf","text":"This is the derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p )","title":"gaussian_pdf"},{"location":"extract_md/param_time_fun/#ln_gaussian_pdf","text":"This is the log of the derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p )","title":"ln_gaussian_pdf"},{"location":"extract_md/param_time_fun/#dgaussian_pdf","text":"This is the second derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{dgaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p )","title":"dgaussian_pdf"},{"location":"extract_md/param_time_fun/#result","text":"The result is a list or one dimensional numpy.array with len(result) == len(t) . If params is a list or one dimensional array result [ i ] = fun ( t [ i ], alpha , beta , p ) If params is a two dimensional array result [ i ] = fun ( t [ i ], alpha [ i ], beta [ i ], p [ i ])","title":"result"},{"location":"extract_md/param_time_fun/#example","text":"param_time_fun_xam","title":"Example"},{"location":"extract_md/param_time_fun_xam/","text":"Example and Test of Predefined Parametric Functions of Time Function Documentation param_time_fun Example Source Code import sys import numpy import scipy import sandbox sandbox . path () import curvefit # eps99 = 99.0 * numpy . finfo ( float ) . eps sqrt_eps = numpy . sqrt ( numpy . finfo ( float ) . eps ) quad_eps = numpy . sqrt ( sqrt_eps ) d_tolerance = 1e-6 # def eval_expit ( t , alpha , beta , p ) : return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # def eval_gaussian_cdf ( t , alpha , beta , p ) : z = alpha * ( t - beta ) return p * ( 1.0 + scipy . special . erf ( z ) ) / 2.0 # # test values for t, alpha, beta, p t = numpy . array ( [ 5.0 , 10.0 ] ) beta = numpy . array ( [ 30.0 , 20.0 ] ) alpha = 2.0 / beta p = numpy . array ( [ 0.1 , 0.2 ] ) params = numpy . vstack ( ( alpha , beta , p ) ) # # check expit value = curvefit . core . functions . expit ( t , params ) check = eval_expit ( t , alpha , beta , p ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check ln_expit value = curvefit . core . functions . ln_expit ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check gaussian_cdf value = curvefit . core . functions . gaussian_cdf ( t , params ) check = eval_gaussian_cdf ( t , alpha , beta , p ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check ln_gaussian_cdf value = curvefit . core . functions . ln_gaussian_cdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check gaussian_pdf step = sqrt_eps * beta value = curvefit . core . functions . gaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - check_m ) / ( 2.0 * step ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # # check ln_gaussian_pdf value = curvefit . core . functions . ln_gaussian_pdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # # check_dgaussian_pdf step = quad_eps * beta value = curvefit . core . functions . dgaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_0 = eval_gaussian_cdf ( t , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - 2.0 * check_0 + check_m ) / step ** 2 rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # print ( 'param_time_fun.py: OK' ) sys . exit ( 0 )","title":"param_time_fun_xam"},{"location":"extract_md/param_time_fun_xam/#example-and-test-of-predefined-parametric-functions-of-time","text":"","title":"Example and Test of Predefined Parametric Functions of Time"},{"location":"extract_md/param_time_fun_xam/#function-documentation","text":"param_time_fun","title":"Function Documentation"},{"location":"extract_md/param_time_fun_xam/#example-source-code","text":"import sys import numpy import scipy import sandbox sandbox . path () import curvefit # eps99 = 99.0 * numpy . finfo ( float ) . eps sqrt_eps = numpy . sqrt ( numpy . finfo ( float ) . eps ) quad_eps = numpy . sqrt ( sqrt_eps ) d_tolerance = 1e-6 # def eval_expit ( t , alpha , beta , p ) : return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # def eval_gaussian_cdf ( t , alpha , beta , p ) : z = alpha * ( t - beta ) return p * ( 1.0 + scipy . special . erf ( z ) ) / 2.0 # # test values for t, alpha, beta, p t = numpy . array ( [ 5.0 , 10.0 ] ) beta = numpy . array ( [ 30.0 , 20.0 ] ) alpha = 2.0 / beta p = numpy . array ( [ 0.1 , 0.2 ] ) params = numpy . vstack ( ( alpha , beta , p ) ) # # check expit value = curvefit . core . functions . expit ( t , params ) check = eval_expit ( t , alpha , beta , p ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check ln_expit value = curvefit . core . functions . ln_expit ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check gaussian_cdf value = curvefit . core . functions . gaussian_cdf ( t , params ) check = eval_gaussian_cdf ( t , alpha , beta , p ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check ln_gaussian_cdf value = curvefit . core . functions . ln_gaussian_cdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # # check gaussian_pdf step = sqrt_eps * beta value = curvefit . core . functions . gaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - check_m ) / ( 2.0 * step ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # # check ln_gaussian_pdf value = curvefit . core . functions . ln_gaussian_pdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # # check_dgaussian_pdf step = quad_eps * beta value = curvefit . core . functions . dgaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_0 = eval_gaussian_cdf ( t , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - 2.0 * check_0 + check_m ) / step ** 2 rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # print ( 'param_time_fun.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/random_effect_xam/","text":"Getting Started Using CurveFit Generalized Logistic Model The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters. Fixed and Random Effects We use the notation a_j , b_j and \\phi_j for the fixed effects, j = 0 , and the random effects j = 1 , \\ldots , n_G . For this example, the link functions, that maps from the fixed and random effects to the parameters, are \\begin{aligned} \\begin{aligned} \\alpha_j & = \\exp \\left( a_0 + a_j \\right) \\\\ \\beta_j & = b_0 + b_j \\\\ p_j & = \\exp \\left( \\phi_0 + \\phi_j \\right) \\\\ \\end{aligned} \\end{aligned} Covariates The constant one is the only covariate in this example. Simulated data Problem Settings The following settings are used to simulate the data and check that the solution is correct: import math n_time = 21 # number of time points used in the simulation n_group = 4 # number of groups rel_tol = 5e-4 # relative tolerance used to check optimal solution # simulation values used for b_0, ..., b_4 b_true = [ 20.0 , - 2.0 , - 1.0 , + 1.0 , + 2.0 ] # simulation values used for a_0, ..., a_4 a_true = [ math . log ( 2.0 ) / b_true [ 0 ], - 0.2 , - 0.1 , + 0.1 , + 0.2 ] # simulation values used for phi_0, ..., phi_4 phi_true = [ math . log ( 0.1 ), - 0.3 , - 0.15 , + 0.15 , + 0.3 ] The fixed effects are initialized to be their true values divided by three. The random effects are initialized to be zero. Time Grid A grid of n_time points in time, t_i , where t_i = b_0 / ( n_t - 1 ) where n_t is the number of time points. The minimum value for this grid is zero and its maximum is b_0 . Measurement values We simulate data, y_{i,j} , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_t - 1 , j = 1 , \\ldots , n_G y_{i,j} = f( t_i , \\alpha_j , \\beta_j , p_j ) There is no noise in this simulated data, but when we do the fitting, we model each data point as having noise. Prior We want our result to fit the noiseless data perfectly, but there is only data for the random effects. We add a prior on the fixed effects to stabilize the estimation procedure. (It would be better if we could specify that the sum of the random effects corresponding to a fixed effect was zero.) Each prior is specified by a mean, equal to the true value, and a standard deviation, equal to 1/100 times the true value. The mean must be the true value for the optimal fit to be perfect. fe_gprior = [ [ a_true [ 0 ], a_true [ 0 ] / 100.0 ], [ b_true [ 0 ], b_true [ 0 ] / 100.0 ], [ phi_true [ 0 ], phi_true [ 0 ] / 100.0 ], ] Example Source Code # ------------------------------------------------------------------------- import scipy import sys import pandas import numpy import sandbox sandbox . path () import curvefit # # number of parameters, fixed effects, random effects num_params = 3 num_fe = 3 num_re = num_fe * n_group # # f(t, alpha, beta, p) def expit ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # # identity function def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # ----------------------------------------------------------------------- # data_frame num_data = n_time * n_group time_grid = numpy . array ( range ( n_time )) * b_true [ 0 ] / ( n_time - 1 ) independent_var = numpy . zeros ( 0 , dtype = float ) measurement_value = numpy . zeros ( 0 , dtype = float ) data_group = list () for j in range ( 1 , n_group + 1 ) : group_j = 'group_' + str ( j ) alpha_j = math . exp ( a_true [ 0 ] + a_true [ j ]) beta_j = b_true [ 0 ] + b_true [ j ] p_j = math . exp ( phi_true [ 0 ] + phi_true [ j ]) y_j = expit ( time_grid , [ alpha_j , beta_j , p_j ]) independent_var = numpy . append ( independent_var , time_grid ) measurement_value = numpy . append ( measurement_value , y_j ) data_group += n_time * [ group_j ] constant_one = num_data * [ 1.0 ] measurement_std = num_data * [ 0.1 ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = num_fe * [ [ 'constant_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = expit col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # # initialize fixed effects so correspond to true parameters divided by three fe_init = numpy . array ( [ a_true [ 0 ], b_true [ 0 ], phi_true [ 0 ] ] ) / 3.0 re_init = numpy . zeros ( num_re ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe options = { 'disp' : 0 , 'maxiter' : 200 , 'ftol' : 1e-8 , 'gtol' : 1e-8 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , fe_gprior , options = options ) fe_estimate = curve_model . result . x [ 0 : num_fe ] re_estimate = curve_model . result . x [ num_fe :] . reshape ( n_group , num_fe ) # ------------------------------------------------------------------------- # check fixed effects fe_truth = [ a_true [ 0 ], b_true [ 0 ], phi_true [ 0 ] ] for i in range ( num_fe ) : rel_error = fe_estimate [ i ] / fe_truth [ i ] - 1.0 assert abs ( rel_error ) < rel_tol for j in range ( n_group ) : re_truth = [ a_true [ j + 1 ], b_true [ j + 1 ], phi_true [ j + 1 ] ] for i in range ( num_fe ) : rel_err = re_estimate [ j , i ] / re_truth [ i ] assert abs ( rel_error ) < rel_tol # print ( 'random_effect.py: OK' ) sys . exit ( 0 )","title":"random_effect_xam"},{"location":"extract_md/random_effect_xam/#getting-started-using-curvefit","text":"","title":"Getting Started Using CurveFit"},{"location":"extract_md/random_effect_xam/#generalized-logistic-model","text":"The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters.","title":"Generalized Logistic Model"},{"location":"extract_md/random_effect_xam/#fixed-and-random-effects","text":"We use the notation a_j , b_j and \\phi_j for the fixed effects, j = 0 , and the random effects j = 1 , \\ldots , n_G . For this example, the link functions, that maps from the fixed and random effects to the parameters, are \\begin{aligned} \\begin{aligned} \\alpha_j & = \\exp \\left( a_0 + a_j \\right) \\\\ \\beta_j & = b_0 + b_j \\\\ p_j & = \\exp \\left( \\phi_0 + \\phi_j \\right) \\\\ \\end{aligned} \\end{aligned}","title":"Fixed and Random Effects"},{"location":"extract_md/random_effect_xam/#covariates","text":"The constant one is the only covariate in this example.","title":"Covariates"},{"location":"extract_md/random_effect_xam/#simulated-data","text":"","title":"Simulated data"},{"location":"extract_md/random_effect_xam/#problem-settings","text":"The following settings are used to simulate the data and check that the solution is correct: import math n_time = 21 # number of time points used in the simulation n_group = 4 # number of groups rel_tol = 5e-4 # relative tolerance used to check optimal solution # simulation values used for b_0, ..., b_4 b_true = [ 20.0 , - 2.0 , - 1.0 , + 1.0 , + 2.0 ] # simulation values used for a_0, ..., a_4 a_true = [ math . log ( 2.0 ) / b_true [ 0 ], - 0.2 , - 0.1 , + 0.1 , + 0.2 ] # simulation values used for phi_0, ..., phi_4 phi_true = [ math . log ( 0.1 ), - 0.3 , - 0.15 , + 0.15 , + 0.3 ] The fixed effects are initialized to be their true values divided by three. The random effects are initialized to be zero.","title":"Problem Settings"},{"location":"extract_md/random_effect_xam/#time-grid","text":"A grid of n_time points in time, t_i , where t_i = b_0 / ( n_t - 1 ) where n_t is the number of time points. The minimum value for this grid is zero and its maximum is b_0 .","title":"Time Grid"},{"location":"extract_md/random_effect_xam/#measurement-values","text":"We simulate data, y_{i,j} , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_t - 1 , j = 1 , \\ldots , n_G y_{i,j} = f( t_i , \\alpha_j , \\beta_j , p_j ) There is no noise in this simulated data, but when we do the fitting, we model each data point as having noise.","title":"Measurement values"},{"location":"extract_md/random_effect_xam/#prior","text":"We want our result to fit the noiseless data perfectly, but there is only data for the random effects. We add a prior on the fixed effects to stabilize the estimation procedure. (It would be better if we could specify that the sum of the random effects corresponding to a fixed effect was zero.) Each prior is specified by a mean, equal to the true value, and a standard deviation, equal to 1/100 times the true value. The mean must be the true value for the optimal fit to be perfect. fe_gprior = [ [ a_true [ 0 ], a_true [ 0 ] / 100.0 ], [ b_true [ 0 ], b_true [ 0 ] / 100.0 ], [ phi_true [ 0 ], phi_true [ 0 ] / 100.0 ], ]","title":"Prior"},{"location":"extract_md/random_effect_xam/#example-source-code","text":"# ------------------------------------------------------------------------- import scipy import sys import pandas import numpy import sandbox sandbox . path () import curvefit # # number of parameters, fixed effects, random effects num_params = 3 num_fe = 3 num_re = num_fe * n_group # # f(t, alpha, beta, p) def expit ( t , params ) : alpha = params [ 0 ] beta = params [ 1 ] p = params [ 2 ] return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ) ) ) # # identity function def identity_fun ( x ) : return x # # link function used for alpha, p def exp_fun ( x ) : return numpy . exp ( x ) # # ----------------------------------------------------------------------- # data_frame num_data = n_time * n_group time_grid = numpy . array ( range ( n_time )) * b_true [ 0 ] / ( n_time - 1 ) independent_var = numpy . zeros ( 0 , dtype = float ) measurement_value = numpy . zeros ( 0 , dtype = float ) data_group = list () for j in range ( 1 , n_group + 1 ) : group_j = 'group_' + str ( j ) alpha_j = math . exp ( a_true [ 0 ] + a_true [ j ]) beta_j = b_true [ 0 ] + b_true [ j ] p_j = math . exp ( phi_true [ 0 ] + phi_true [ j ]) y_j = expit ( time_grid , [ alpha_j , beta_j , p_j ]) independent_var = numpy . append ( independent_var , time_grid ) measurement_value = numpy . append ( measurement_value , y_j ) data_group += n_time * [ group_j ] constant_one = num_data * [ 1.0 ] measurement_std = num_data * [ 0.1 ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model col_t = 'independent_var' col_obs = 'measurement_value' col_covs = num_fe * [ [ 'constant_one' ] ] col_group = 'data_group' param_names = [ 'alpha' , 'beta' , 'p' ] link_fun = [ exp_fun , identity_fun , exp_fun ] var_link_fun = num_fe * [ identity_fun ] fun = expit col_obs_se = 'measurement_std' # curve_model = curvefit . core . model . CurveModel ( data_frame , col_t , col_obs , col_covs , col_group , param_names , link_fun , var_link_fun , fun , col_obs_se ) # ------------------------------------------------------------------------- # fit_params # # initialize fixed effects so correspond to true parameters divided by three fe_init = numpy . array ( [ a_true [ 0 ], b_true [ 0 ], phi_true [ 0 ] ] ) / 3.0 re_init = numpy . zeros ( num_re ) fe_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe re_bounds = [ [ - numpy . inf , numpy . inf ] ] * num_fe options = { 'disp' : 0 , 'maxiter' : 200 , 'ftol' : 1e-8 , 'gtol' : 1e-8 , } # curve_model . fit_params ( fe_init , re_init , fe_bounds , re_bounds , fe_gprior , options = options ) fe_estimate = curve_model . result . x [ 0 : num_fe ] re_estimate = curve_model . result . x [ num_fe :] . reshape ( n_group , num_fe ) # ------------------------------------------------------------------------- # check fixed effects fe_truth = [ a_true [ 0 ], b_true [ 0 ], phi_true [ 0 ] ] for i in range ( num_fe ) : rel_error = fe_estimate [ i ] / fe_truth [ i ] - 1.0 assert abs ( rel_error ) < rel_tol for j in range ( n_group ) : re_truth = [ a_true [ j + 1 ], b_true [ j + 1 ], phi_true [ j + 1 ] ] for i in range ( num_fe ) : rel_err = re_estimate [ j , i ] / re_truth [ i ] assert abs ( rel_error ) < rel_tol # print ( 'random_effect.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/sizes_to_indices/","text":"Converting sizes to corresponding indices. Syntax indices = curvefit.sizes_to_indices(sizes) sizes The argument sizes is an iterable object with integer values. The i-th value in sizes[i] is the number of elements in the i-th subvector of a larger total vector that contains the subvectors in order. indices The return value indices is a list of one dimensional numpy arrays. The value indices[i] has length equal to the i-th size. It starts (ends) with the index in the total vector of the first (last) element of the i-th subvector. The elements of indices[i] are monotone and increase by one between elements. Example sizes_to_indices_xam","title":"sizes_to_indices"},{"location":"extract_md/sizes_to_indices/#converting-sizes-to-corresponding-indices","text":"","title":"Converting sizes to corresponding indices."},{"location":"extract_md/sizes_to_indices/#syntax","text":"indices = curvefit.sizes_to_indices(sizes)","title":"Syntax"},{"location":"extract_md/sizes_to_indices/#sizes","text":"The argument sizes is an iterable object with integer values. The i-th value in sizes[i] is the number of elements in the i-th subvector of a larger total vector that contains the subvectors in order.","title":"sizes"},{"location":"extract_md/sizes_to_indices/#indices","text":"The return value indices is a list of one dimensional numpy arrays. The value indices[i] has length equal to the i-th size. It starts (ends) with the index in the total vector of the first (last) element of the i-th subvector. The elements of indices[i] are monotone and increase by one between elements.","title":"indices"},{"location":"extract_md/sizes_to_indices/#example","text":"sizes_to_indices_xam","title":"Example"},{"location":"extract_md/sizes_to_indices_xam/","text":"Example and Test of sizes_to_indices Function Documentation size_to_indices Example Source Code import sys import numpy import sandbox sandbox . path () import curvefit # sizes = [ 2 , 4 , 3 ] indices = curvefit . core . utils . sizes_to_indices ( sizes ) assert all ( indices [ 0 ] == numpy . array ([ 0 , 1 ]) ) assert all ( indices [ 1 ] == numpy . array ([ 2 , 3 , 4 , 5 ]) ) assert all ( indices [ 2 ] == numpy . array ([ 6 , 7 , 8 ]) ) print ( 'sizes_to_indices.py: OK' ) sys . exit ( 0 )","title":"sizes_to_indices_xam"},{"location":"extract_md/sizes_to_indices_xam/#example-and-test-of-sizes_to_indices","text":"","title":"Example and Test of sizes_to_indices"},{"location":"extract_md/sizes_to_indices_xam/#function-documentation","text":"size_to_indices","title":"Function Documentation"},{"location":"extract_md/sizes_to_indices_xam/#example-source-code","text":"import sys import numpy import sandbox sandbox . path () import curvefit # sizes = [ 2 , 4 , 3 ] indices = curvefit . core . utils . sizes_to_indices ( sizes ) assert all ( indices [ 0 ] == numpy . array ([ 0 , 1 ]) ) assert all ( indices [ 1 ] == numpy . array ([ 2 , 3 , 4 , 5 ]) ) assert all ( indices [ 2 ] == numpy . array ([ 6 , 7 , 8 ]) ) print ( 'sizes_to_indices.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/split_by_group/","text":"Split the dataframe by the group definition. Syntax data = split_by_group(df, col_group) df Provided dataframe. col_group Column name in the dataframe contains group definition. data Dictionary with key as the group definition and value as the corresponding dataframe. Example","title":"split_by_group"},{"location":"extract_md/split_by_group/#split-the-dataframe-by-the-group-definition","text":"","title":"Split the dataframe by the group definition."},{"location":"extract_md/split_by_group/#syntax","text":"data = split_by_group(df, col_group)","title":"Syntax"},{"location":"extract_md/split_by_group/#df","text":"Provided dataframe.","title":"df"},{"location":"extract_md/split_by_group/#col_group","text":"Column name in the dataframe contains group definition.","title":"col_group"},{"location":"extract_md/split_by_group/#data","text":"Dictionary with key as the group definition and value as the corresponding dataframe.","title":"data"},{"location":"extract_md/split_by_group/#example","text":"","title":"Example"},{"location":"extract_md/st_loss/","text":"Student's t Loss Function Syntax loss = curvefit.core.loss_fun.st_loss(r, nu = 1.0) t is a numpy vector of residual values. We use n for the length of the vector. The elements of this vector can be float or a_double values. nu is the number of degrees of freedom in the t distribution \\nu . This can be a float or a_double value. Distribution The student's t-distribution is f(r) = ( 1 + r^2 / \\nu )^{- (\\nu + 1) / 2 } \\Gamma[ ( \\nu + 1) / 2 ] / [ \\sqrt{ \\nu \\pi } \\Gamma( \\nu / 2 ) ] where \\nu is the number of degrees of freedom and \\Gamma is the gamma function. Negative log Taking the negative log of the distribution function we get - \\log [ f(r) ] = \\log ( 1 + r^2 / \\nu ) (\\nu + 1) / 2 + c where c is constant w.r.t. r . loss The return value loss is a scalar equal to \\frac{\\nu + 1}{2} \\sum_{i=0}^{n-1} \\log( 1 + r_i^2 / \\nu ) Example loss_xam","title":"st_loss"},{"location":"extract_md/st_loss/#students-t-loss-function","text":"","title":"Student's t Loss Function"},{"location":"extract_md/st_loss/#syntax","text":"loss = curvefit.core.loss_fun.st_loss(r, nu = 1.0)","title":"Syntax"},{"location":"extract_md/st_loss/#t","text":"is a numpy vector of residual values. We use n for the length of the vector. The elements of this vector can be float or a_double values.","title":"t"},{"location":"extract_md/st_loss/#nu","text":"is the number of degrees of freedom in the t distribution \\nu . This can be a float or a_double value.","title":"nu"},{"location":"extract_md/st_loss/#distribution","text":"The student's t-distribution is f(r) = ( 1 + r^2 / \\nu )^{- (\\nu + 1) / 2 } \\Gamma[ ( \\nu + 1) / 2 ] / [ \\sqrt{ \\nu \\pi } \\Gamma( \\nu / 2 ) ] where \\nu is the number of degrees of freedom and \\Gamma is the gamma function.","title":"Distribution"},{"location":"extract_md/st_loss/#negative-log","text":"Taking the negative log of the distribution function we get - \\log [ f(r) ] = \\log ( 1 + r^2 / \\nu ) (\\nu + 1) / 2 + c where c is constant w.r.t. r .","title":"Negative log"},{"location":"extract_md/st_loss/#loss","text":"The return value loss is a scalar equal to \\frac{\\nu + 1}{2} \\sum_{i=0}^{n-1} \\log( 1 + r_i^2 / \\nu )","title":"loss"},{"location":"extract_md/st_loss/#example","text":"loss_xam","title":"Example"},{"location":"extract_md/unzip_x/","text":"Extract Fixed and Random Effects from Single Vector Form Syntax fe, re = curvefit.core.effects2params.unzip_x(x, num_groups, num_fe) num_groups is the number of data groups. num_fe is the number of fixed effects. x is a numpy vector with length equal to (num_groups + 1)*num_fe fe this return value is a numpy vector containing the fist num_fe elements of x . re this return value is a numpy two dimensional array with row dimension num_groups and column dimension num_fe . The i-th row of re contains the following sub-vector of x re [ i ,:] = x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ] Example unzip_x_xam","title":"unzip_x"},{"location":"extract_md/unzip_x/#extract-fixed-and-random-effects-from-single-vector-form","text":"","title":"Extract Fixed and Random Effects from Single Vector Form"},{"location":"extract_md/unzip_x/#syntax","text":"fe, re = curvefit.core.effects2params.unzip_x(x, num_groups, num_fe)","title":"Syntax"},{"location":"extract_md/unzip_x/#num_groups","text":"is the number of data groups.","title":"num_groups"},{"location":"extract_md/unzip_x/#num_fe","text":"is the number of fixed effects.","title":"num_fe"},{"location":"extract_md/unzip_x/#x","text":"is a numpy vector with length equal to (num_groups + 1)*num_fe","title":"x"},{"location":"extract_md/unzip_x/#fe","text":"this return value is a numpy vector containing the fist num_fe elements of x .","title":"fe"},{"location":"extract_md/unzip_x/#re","text":"this return value is a numpy two dimensional array with row dimension num_groups and column dimension num_fe . The i-th row of re contains the following sub-vector of x re [ i ,:] = x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ]","title":"re"},{"location":"extract_md/unzip_x/#example","text":"unzip_x_xam","title":"Example"},{"location":"extract_md/unzip_x_xam/","text":"Example and Test of unzip_x Function Documentation unzip_x Example Source Code import sys import numpy import sandbox sandbox . path () import curvefit # num_groups = 2 num_fe = 3 x = numpy . array ( range ( ( num_groups + 1 ) * num_fe ) ) fe , re = curvefit . core . effects2params . unzip_x ( x , num_groups , num_fe ) assert fe . ndim == 1 assert re . ndim == 2 assert fe . shape [ 0 ] == num_fe assert re . shape [ 0 ] == num_groups assert re . shape [ 1 ] == num_fe assert all ( fe == x [ 0 : num_fe ] ) for i in range ( num_groups ) : assert all ( re [ i ,:] == x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ] ) print ( 'unzip_x.py: OK' ) sys . exit ( 0 )","title":"unzip_x_xam"},{"location":"extract_md/unzip_x_xam/#example-and-test-of-unzip_x","text":"","title":"Example and Test of unzip_x"},{"location":"extract_md/unzip_x_xam/#function-documentation","text":"unzip_x","title":"Function Documentation"},{"location":"extract_md/unzip_x_xam/#example-source-code","text":"import sys import numpy import sandbox sandbox . path () import curvefit # num_groups = 2 num_fe = 3 x = numpy . array ( range ( ( num_groups + 1 ) * num_fe ) ) fe , re = curvefit . core . effects2params . unzip_x ( x , num_groups , num_fe ) assert fe . ndim == 1 assert re . ndim == 2 assert fe . shape [ 0 ] == num_fe assert re . shape [ 0 ] == num_groups assert re . shape [ 1 ] == num_fe assert all ( fe == x [ 0 : num_fe ] ) for i in range ( num_groups ) : assert all ( re [ i ,:] == x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ] ) print ( 'unzip_x.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"}]}